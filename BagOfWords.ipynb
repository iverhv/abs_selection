{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain keywords - feature set\n",
    "\n",
    "\n",
    "1. load the test set\n",
    "2. Load the pickle-object\n",
    "3. Hyperparameter tuning \n",
    "    1. Generate the initial training split\n",
    "    2. Classical \n",
    "        - Support Vector machine (SVM)\n",
    "        - Logistic Regression (LR)\n",
    "        - Random Forest (RF)\n",
    "    3. Shallow deep learning models\n",
    "        - Tuned using optuna\n",
    "4. Model evaluation\n",
    "    1. Classical\n",
    "    2. Shallow deep learning models\n",
    "        - Priorly tuned model\n",
    "        - Optuna hyperparameter set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.5 Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting AUC curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# keras/tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_addons as tfa\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# skearln\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, make_scorer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# ml models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm, tree\n",
    "from sklearn import model_selection\n",
    "\n",
    "# optimizing models\n",
    "import optuna\n",
    "# visualizing optuna loss\n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "import en_core_web_trf\n",
    "import en_core_web_sm\n",
    "\n",
    "# plotting AUC curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# scikeras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# stats\n",
    "from scipy.stats import sem\n",
    "\n",
    "# saving\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def average_loss(cv_results):\n",
    "\n",
    "    loss_list = []; val_loss_list = []\n",
    "\n",
    "    for i in cv_results['estimator']:\n",
    "\n",
    "        loss_list.append(i.history_['loss'])\n",
    "        val_loss_list.append(i.history_['val_loss'])\n",
    "\n",
    "    loss_array = np.array(loss_list); val_loss_array = np.array(val_loss_list)\n",
    "    average_loss = loss_array.mean(axis=0); average_val_loss = val_loss_array.mean(axis=0)\n",
    "\n",
    "    plt.plot(average_loss); plt.plot(average_val_loss)\n",
    "    plt.ylim([0.0, 50])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def extract_hyper_results(df, param1, param2):\n",
    "    \n",
    "    slice1 = df.loc[ ('set1', param1, param2),:]\n",
    "    #slice2 = df.loc[('set2',   param1, param2),:]\n",
    "    #slice3 = df.loc[('set3',   param1, param2),:]\n",
    "    #slice4 = df.loc[('set4',   param1, param2),:]\n",
    "    #slice5 = df.loc[('set5',   param1, param2),:]\n",
    "\n",
    "    return pd.DataFrame(data=[slice1]).round(decimals=3)\n",
    "\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    return re.sub(pattern,'',text) \n",
    "\n",
    "def remove_elsevier_stuff(text):\n",
    "    # attmepting to remove stuff journals add to all abstracts, is too many different stuff, and I couldn't find a common regex expression to remove them all. \n",
    "    pattern = r'\\©'\n",
    "    pattern1 = r'\\©\\s\\d{1,4}\\sElsevier\\sLtd'\n",
    "    pattern2 = r'\\©\\s\\d{1,4}\\sElsevier\\sB\\.V\\.'\n",
    "    pattern3 = r'\\©\\s\\d{0,4}\\sElsevier\\s[\\.A-Za-z]*\\sAll\\srights\\sreserved.'\n",
    "    pattern4 = r'\\©\\s\\d{1,4}\\sMoscoviz\\set\\sal\\.Background\\:\\s'\n",
    "    pattern5 = r'\\©\\s\\d{1,4}\\,* Akadémiai Kiadó, Budapest, Hungary.'\n",
    "    pattern6 = r'\\©\\s\\d{1,4}\\sAmerican Institute of Chemical Engineers[Chemical Engineers\\sBiotechnol\\. Prog\\.\\,\\d{1,3}\\:\\d{1,3}\\–\\d{1,3}\\, \\d{1,4}\\.]*'\n",
    "    pattern7 = r'^\\©\\s\\d{1,4}\\s*Wiley-VCH Verlag GmbH & Co\\. KGaA, Weinheim'\n",
    "    pattern8 = r'\\©\\s\\d{1,4}\\sThe Authors'\n",
    "    pattern9 = r'\\©\\s\\d{1,4}\\sWILEY-VCH Verlag GmbH & Co\\. KGaA\\.'\n",
    "    pattern10 = r'\\©\\s\\d{1,4}\\sAOCS'\n",
    "    pattern11 = r'\\©\\s\\d{1,4}\\sTaylor \\& Francis Group\\, LLC\\.'\n",
    "    \n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "def remove_punct_and_short_words(text):\n",
    "\n",
    "    not_important_pos = ['PUNCT', 'NUM']\n",
    "\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    new_token_list = []\n",
    "    for token in doc:\n",
    "        #print(token, token.pos_, token.dep_)\n",
    "        if not token.pos_ in not_important_pos:\n",
    "            if len(token.text) > 2:\n",
    "                new_token_list.append(token.text)  \n",
    "            \n",
    "    return ' '.join(new_token_list)\n",
    "\n",
    "import spacy #load spacy\n",
    "\n",
    "not_important_pos = ['PUNCT', 'NUM']\n",
    "nlp = en_core_web_lg.load()\n",
    "stops = nlp.Defaults.stop_words\n",
    "#stops = stopwords.words(\"english\")\n",
    "def remove_copyright_and_year(text):\n",
    "    \n",
    "    pattern = r'©\\s*\\d{,4}'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = remove_copyright_and_year(comment)\n",
    "    comment = nlp(comment)\n",
    "\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "\n",
    "                lemmatized.append(lemma)\n",
    "                \n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "\n",
    "#Data['Text_After_Clean'] = Data['Text'].apply(normalize, lowercase=True, remove_stopwords=True)\n",
    "\n",
    "\n",
    "def run_exps(X_train: pd.DataFrame ,\n",
    "             y_train: pd.DataFrame,\n",
    "             X_test: pd.DataFrame,\n",
    "             y_test: pd.DataFrame, X, y) -> pd.DataFrame:\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners:param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    models = [\n",
    "          ('LogReg', LogisticRegression(max_iter=300)), \n",
    "          ('RF', RandomForestClassifier()),\n",
    "          #('KNN', KNeighborsClassifier()),\n",
    "          ('SVM', SVC()),\n",
    "\n",
    "        ]\n",
    "    results = []\n",
    "    names = []\n",
    "    \n",
    "    scoring = {'accuracy': 'accuracy',\n",
    "           'recall': 'recall',\n",
    "           'precision': 'precision',\n",
    "           'mcc': 'matthews_corrcoef',\n",
    "           'auc': 'roc_auc',\n",
    "           'f1_pos': 'f1',\n",
    "           'f1_neg': make_scorer(f1_score, pos_label=0, average='binary')\n",
    "          }\n",
    "    \n",
    "    #scoring = ['accuracy', 'recall', 'precision', 'f1']\n",
    "    target_names = ['Relevant', 'Irrelevant']\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    for name, model in models:\n",
    "        kfold = model_selection.RepeatedStratifiedKFold(n_splits=4, n_repeats=5, random_state=1)\n",
    "        cv_results = model_selection.cross_validate(model, X, y, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        train_pred = clf.predict(X_train)\n",
    "        print(name)\n",
    "        res = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "        \n",
    "        print('train')\n",
    "        print(classification_report(y_train, train_pred, target_names=target_names))\n",
    "        print('MCC', matthews_corrcoef(y_train, train_pred))\n",
    "        print('AUC', roc_auc_score(y_train, train_pred))\n",
    "        print('test')\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        print('MCC', matthews_corrcoef(y_test, y_pred))\n",
    "        print('AUC', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "        result_dict[name] = res\n",
    "        \n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        this_df = pd.DataFrame(cv_results)\n",
    "        this_df['model'] = name\n",
    "        dfs.append(this_df)\n",
    "        \n",
    "        final = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return final, result_dict\n",
    "\n",
    "def ann_metrics(model, features, labels):\n",
    "    \n",
    "    y_pred = model.predict(features.astype('float64'), batch_size=1, verbose=0)\n",
    "    #print(y_pred)\n",
    "    \n",
    "    y_pred_bool_manual = []\n",
    "    \n",
    "    for i in y_pred:\n",
    "        if i>0.3:\n",
    "            y_pred_bool_manual.append(1)\n",
    "        else:\n",
    "            y_pred_bool_manual.append(0)\n",
    "    #print(y_pred_bool_manual)\n",
    "    target_names = ['Irrelevant', 'Relevant']\n",
    "    res = classification_report(labels, y_pred_bool_manual, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    print(classification_report(labels, y_pred_bool_manual, target_names=target_names, zero_division=1))\n",
    "    \n",
    "def mcc_metric(y_true, y_pred):\n",
    "    predicted = tf.cast(tf.greater(y_pred, threshold), tf.float32)\n",
    "    true_pos = tf.math.count_nonzero(predicted * y_true)\n",
    "    true_neg = tf.math.count_nonzero((predicted - 1) * (y_true - 1))\n",
    "    false_pos = tf.math.count_nonzero(predicted * (y_true - 1))\n",
    "    false_neg = tf.math.count_nonzero((predicted - 1) * y_true)\n",
    "    x = tf.cast((true_pos + false_pos) * (true_pos + false_neg) \n",
    "      * (true_neg + false_pos) * (true_neg + false_neg), tf.float32)\n",
    "    \n",
    "    hei = tf.cast((true_pos * true_neg) - (false_pos * false_neg), tf.float32) / tf.sqrt(x)\n",
    "    print(hei)\n",
    "    return hei\n",
    "\n",
    "def mcc_threshold(threshold):\n",
    "    def mcc_metric2(y_true, y_pred):\n",
    "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold), K.floatx())\n",
    "\n",
    "        tp = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "        tn = K.round(K.sum(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "        fp = K.round(K.sum(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "        fn = K.round(K.sum(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "        num = tp * tn - fp * fn\n",
    "        den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "        return num / K.sqrt(den + K.epsilon())\n",
    "    return mcc_metric2\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.ylim([0, 2])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [relevant]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "def plot_auc_curve(x_train, x_test, y_train, y_test):\n",
    "\n",
    "    y_test_pred = model_8.predict(x_test.toarray())\n",
    "    y_train_pred = model_8.predict(x_train.toarray())\n",
    "\n",
    "    fpr_test, tpr_test, thresholds_keras = roc_curve(y_test, y_test_pred)\n",
    "    fpr_train, tpr_train, thresholds_keras = roc_curve(y_train, y_train_pred)\n",
    "\n",
    "    auc_test = auc(fpr_test, tpr_test)\n",
    "    auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_test, tpr_test, label='train (area = {:.3f})'.format(auc_test))\n",
    "    plt.plot(fpr_train, tpr_train, label='test (area = {:.3f})'.format(auc_train))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    #plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def scale_mcc(mcc):\n",
    "    return ((mcc-(-1))/(1-(-1)))*(1-0)+0\n",
    "\n",
    "\n",
    "def custom_f1_pos(y_true, y_pred):\n",
    "    def recall_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "        recall = TP / (Positives+K.epsilon())\n",
    "        return recall\n",
    "\n",
    "\n",
    "    def precision_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "        precision = TP / (Pred_Positives+K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def custom_f1_neg(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tn / (tn + fn + K.epsilon())\n",
    "    r = tn / (tn + fp + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 2*((p*r)/(p+r+K.epsilon()))\n",
    "\n",
    "def calculate_f1_score(y, pred, threshold, pos_label):\n",
    "    # if this also wrong, I compute myself\n",
    "    y_pred = pred.copy()\n",
    "\n",
    "    y_pred[y_pred > threshold] = 1\n",
    "    y_pred[y_pred < threshold] = 0\n",
    "\n",
    "    return f1_score(y, y_pred, pos_label=pos_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016/j.cattod.2019.08.027</td>\n",
       "      <td>© 2019 Elsevier B.V.Sodium titanate nanotubes ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.fuel.2021.120653</td>\n",
       "      <td>© 2021 Elsevier LtdThis research investigated ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.fuel.2014.07.035</td>\n",
       "      <td>This research reports on the synthesis of meth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.enconman.2017.03.044</td>\n",
       "      <td>© 2017 Elsevier LtdAn optimization of methyl e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.supflu.2011.11.012</td>\n",
       "      <td>This work reports phase equilibrium measuremen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>10.1016/j.fuproc.2010.05.032</td>\n",
       "      <td>In this study, biodiesel was produced from Mor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>10.1016/j.combustflame.2013.09.013</td>\n",
       "      <td>The oxidation characteristics of several small...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>10.1016/j.algal.2015.09.006</td>\n",
       "      <td>The feasibility of microalgae based biodiesel ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>10.1016/j.biortech.2016.05.020</td>\n",
       "      <td>Mixotrophic growth of microalgae to boost lipi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>10.1016/j.energy.2020.119671</td>\n",
       "      <td>Given the scarceness of fossil fuels and high ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    doi  \\\n",
       "0          10.1016/j.cattod.2019.08.027   \n",
       "1            10.1016/j.fuel.2021.120653   \n",
       "2            10.1016/j.fuel.2014.07.035   \n",
       "3        10.1016/j.enconman.2017.03.044   \n",
       "4          10.1016/j.supflu.2011.11.012   \n",
       "..                                  ...   \n",
       "495        10.1016/j.fuproc.2010.05.032   \n",
       "496  10.1016/j.combustflame.2013.09.013   \n",
       "497         10.1016/j.algal.2015.09.006   \n",
       "498      10.1016/j.biortech.2016.05.020   \n",
       "499        10.1016/j.energy.2020.119671   \n",
       "\n",
       "                                              abstract  label  \n",
       "0    © 2019 Elsevier B.V.Sodium titanate nanotubes ...      0  \n",
       "1    © 2021 Elsevier LtdThis research investigated ...      1  \n",
       "2    This research reports on the synthesis of meth...      1  \n",
       "3    © 2017 Elsevier LtdAn optimization of methyl e...      1  \n",
       "4    This work reports phase equilibrium measuremen...      1  \n",
       "..                                                 ...    ...  \n",
       "495  In this study, biodiesel was produced from Mor...      1  \n",
       "496  The oxidation characteristics of several small...      0  \n",
       "497  The feasibility of microalgae based biodiesel ...      0  \n",
       "498  Mixotrophic growth of microalgae to boost lipi...      0  \n",
       "499  Given the scarceness of fossil fuels and high ...      1  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_json('data/new_test_set_II.json')\n",
    "test = pd.read_json('data/validation_set.json')\n",
    "\n",
    "test_set1 = test_set.rename(columns={'DOI': 'doi', 'Abstract': 'abstract', 'Label':'label'})\n",
    "\n",
    "full_set = pd.concat([test_set1, test]).reset_index(drop=True)\n",
    "\n",
    "full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1 = pd.read_json('data/new_test_set_II.json')\n",
    "training2 = pd.read_json('data/validation_set.json')\n",
    "training1 = training1.rename(columns={'DOI':'doi', 'Abstract': 'abstract', 'Label':'label'})\n",
    "\n",
    "train = pd.concat([training1, training2]).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load the pickle object\n",
    "\n",
    "generated with CountVectorizer in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.1.3 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vec = pickle.load(open('bow_features/vector.pickel', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = train.copy()\n",
    "cleaned_df['abstract'] = cleaned_df['abstract'].apply(denoise_text)\n",
    "cleaned_df['abstract'] = cleaned_df['abstract'].apply(remove_special_characters)\n",
    "cleaned_df['abstract'] = cleaned_df['abstract'].apply(normalize, lowercase=True, remove_stopwords=True)\n",
    "cleaned_df['abstract'] = cleaned_df['abstract'].apply(remove_punct_and_short_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016/j.cattod.2019.08.027</td>\n",
       "      <td>elsevier bvsodium titanate nanotubes stn modif...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.fuel.2021.120653</td>\n",
       "      <td>elsevier ltdthis research investigate water in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.fuel.2014.07.035</td>\n",
       "      <td>research report synthesis methylic ethylic bio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.enconman.2017.03.044</td>\n",
       "      <td>elsevier ltdan optimization methyl ester synth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.supflu.2011.11.012</td>\n",
       "      <td>work report phase equilibrium measurement tern...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>10.1016/j.fuproc.2010.05.032</td>\n",
       "      <td>study biodiesel produce moringa oleifera oil u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>10.1016/j.combustflame.2013.09.013</td>\n",
       "      <td>oxidation characteristic small methyl ethyl es...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>10.1016/j.algal.2015.09.006</td>\n",
       "      <td>feasibility microalgae base biodiesel depend s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>10.1016/j.biortech.2016.05.020</td>\n",
       "      <td>mixotrophic growth microalgae boost lipid prod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>10.1016/j.energy.2020.119671</td>\n",
       "      <td>scarceness fossil fuel high volume pollution c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    doi  \\\n",
       "0          10.1016/j.cattod.2019.08.027   \n",
       "1            10.1016/j.fuel.2021.120653   \n",
       "2            10.1016/j.fuel.2014.07.035   \n",
       "3        10.1016/j.enconman.2017.03.044   \n",
       "4          10.1016/j.supflu.2011.11.012   \n",
       "..                                  ...   \n",
       "495        10.1016/j.fuproc.2010.05.032   \n",
       "496  10.1016/j.combustflame.2013.09.013   \n",
       "497         10.1016/j.algal.2015.09.006   \n",
       "498      10.1016/j.biortech.2016.05.020   \n",
       "499        10.1016/j.energy.2020.119671   \n",
       "\n",
       "                                              abstract  label  \n",
       "0    elsevier bvsodium titanate nanotubes stn modif...      0  \n",
       "1    elsevier ltdthis research investigate water in...      1  \n",
       "2    research report synthesis methylic ethylic bio...      1  \n",
       "3    elsevier ltdan optimization methyl ester synth...      1  \n",
       "4    work report phase equilibrium measurement tern...      1  \n",
       "..                                                 ...    ...  \n",
       "495  study biodiesel produce moringa oleifera oil u...      1  \n",
       "496  oxidation characteristic small methyl ethyl es...      0  \n",
       "497  feasibility microalgae base biodiesel depend s...      0  \n",
       "498  mixotrophic growth microalgae boost lipid prod...      0  \n",
       "499  scarceness fossil fuel high volume pollution c...      1  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Hyperparameter tuning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Classical models\n",
    "\n",
    "### 3.1.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model and define the hyperparameter range(s)\n",
    "# SVM\n",
    "\n",
    "C = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "gamma = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "svm_dict = {}; index = 0\n",
    "\n",
    "# divide into X and y\n",
    "x = cleaned_df['abstract'].values\n",
    "y = cleaned_df['label'].values\n",
    "\n",
    "# convert X into the features\n",
    "X = vec.transform(x).toarray()\n",
    "\n",
    "res_list = []\n",
    "\n",
    "for c in C:\n",
    "\n",
    "    for g in gamma:\n",
    "        res_dict = {'C':float, 'gamma': float, 'metric': {}}\n",
    "        single_res = {'train_accuracy': float, 'test_accuracy': float, 'train_recall': float, 'test_recall': float, 'train_precison': float,\n",
    "                        'test_precison': float,'train_auc': float, 'test_auc': float, 'train_f1_pos':float, 'test_f1_pos': float, 'train_f1_neg': float, 'test_f1_neg': float}\n",
    "\n",
    "\n",
    "        clf = SVC(C=c, gamma=g)\n",
    "\n",
    "        # RepeatedStratifiedKfold\n",
    "\n",
    "        kfold = model_selection.RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=1)\n",
    "\n",
    "        scoring = {'accuracy': 'accuracy',\n",
    "                    'recall': 'recall',\n",
    "                    'precison': 'precision',\n",
    "                    'auc': 'roc_auc',\n",
    "                    'f1_pos': 'f1',\n",
    "                    'f1_neg': make_scorer(f1_score, pos_label=0, average='binary')\n",
    "                    }  \n",
    "\n",
    "        cv_results = model_selection.cross_validate(clf, X, y, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "\n",
    "        for metric, values in cv_results.items():\n",
    "\n",
    "            single_res[metric] = (np.round(values.mean(), decimals=3), np.round(values.std(), decimals=3))\n",
    "            \n",
    "\n",
    "        \n",
    "        res_dict['C'] = c\n",
    "        res_dict['gamma'] = g\n",
    "        res_dict['metric'] = single_res\n",
    "\n",
    "        res_list.append(res_dict)\n",
    "\n",
    "svm_dict['set' + str(index + 1)] = res_list\n",
    "\n",
    "hyper_list = []\n",
    "data_list = []\n",
    "\n",
    "for j, res in svm_dict.items():\n",
    "    \n",
    "    for i in res:\n",
    "        \n",
    "        hyper_list.append((j, i['C'], i['gamma']))\n",
    "\n",
    "        data_list.append(i['metric'])\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(hyper_list, names=['train/valset', 'C', 'gamma'])\n",
    "svm_df = pd.DataFrame(data_list, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_mcc</th>\n",
       "      <th>test_mcc</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_f1_pos</th>\n",
       "      <th>test_f1_pos</th>\n",
       "      <th>train_f1_neg</th>\n",
       "      <th>test_f1_neg</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>set1</th>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>(0.904, 0.006)</td>\n",
       "      <td>(0.859, 0.032)</td>\n",
       "      <td>(0.896, 0.011)</td>\n",
       "      <td>(0.842, 0.049)</td>\n",
       "      <td>(0.904, 0.006)</td>\n",
       "      <td>(0.86, 0.032)</td>\n",
       "      <td>(0.96, 0.004)</td>\n",
       "      <td>(0.924, 0.025)</td>\n",
       "      <td>(0.902, 0.006)</td>\n",
       "      <td>(0.854, 0.034)</td>\n",
       "      <td>(0.907, 0.005)</td>\n",
       "      <td>(0.863, 0.031)</td>\n",
       "      <td>(0.059, 0.008)</td>\n",
       "      <td>(0.065, 0.008)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train_accuracy   test_accuracy    train_recall  \\\n",
       "set1 100.0 0.00001  (0.904, 0.006)  (0.859, 0.032)  (0.896, 0.011)   \n",
       "\n",
       "                       test_recall       train_mcc       test_mcc  \\\n",
       "set1 100.0 0.00001  (0.842, 0.049)  (0.904, 0.006)  (0.86, 0.032)   \n",
       "\n",
       "                        train_auc        test_auc    train_f1_pos  \\\n",
       "set1 100.0 0.00001  (0.96, 0.004)  (0.924, 0.025)  (0.902, 0.006)   \n",
       "\n",
       "                       test_f1_pos    train_f1_neg     test_f1_neg  \\\n",
       "set1 100.0 0.00001  (0.854, 0.034)  (0.907, 0.005)  (0.863, 0.031)   \n",
       "\n",
       "                          fit_time      score_time  \n",
       "set1 100.0 0.00001  (0.059, 0.008)  (0.065, 0.008)  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_hyper_results(svm_df, 100, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_mcc</th>\n",
       "      <th>test_mcc</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_f1_pos</th>\n",
       "      <th>test_f1_pos</th>\n",
       "      <th>train_f1_neg</th>\n",
       "      <th>test_f1_neg</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>set1</th>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>(0.886, 0.01)</td>\n",
       "      <td>(0.836, 0.037)</td>\n",
       "      <td>(0.882, 0.02)</td>\n",
       "      <td>(0.841, 0.048)</td>\n",
       "      <td>(0.886, 0.01)</td>\n",
       "      <td>(0.836, 0.038)</td>\n",
       "      <td>(0.952, 0.006)</td>\n",
       "      <td>(0.904, 0.03)</td>\n",
       "      <td>(0.885, 0.011)</td>\n",
       "      <td>(0.836, 0.038)</td>\n",
       "      <td>(0.887, 0.009)</td>\n",
       "      <td>(0.835, 0.038)</td>\n",
       "      <td>(0.024, 0.009)</td>\n",
       "      <td>(0.037, 0.009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set2</th>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>(0.92, 0.007)</td>\n",
       "      <td>(0.879, 0.026)</td>\n",
       "      <td>(0.896, 0.012)</td>\n",
       "      <td>(0.856, 0.045)</td>\n",
       "      <td>(0.92, 0.007)</td>\n",
       "      <td>(0.88, 0.026)</td>\n",
       "      <td>(0.968, 0.004)</td>\n",
       "      <td>(0.938, 0.02)</td>\n",
       "      <td>(0.916, 0.008)</td>\n",
       "      <td>(0.874, 0.027)</td>\n",
       "      <td>(0.923, 0.007)</td>\n",
       "      <td>(0.883, 0.025)</td>\n",
       "      <td>(0.019, 0.008)</td>\n",
       "      <td>(0.031, 0.006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set3</th>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>(0.903, 0.007)</td>\n",
       "      <td>(0.859, 0.031)</td>\n",
       "      <td>(0.886, 0.015)</td>\n",
       "      <td>(0.836, 0.055)</td>\n",
       "      <td>(0.903, 0.007)</td>\n",
       "      <td>(0.861, 0.031)</td>\n",
       "      <td>(0.957, 0.006)</td>\n",
       "      <td>(0.918, 0.027)</td>\n",
       "      <td>(0.901, 0.008)</td>\n",
       "      <td>(0.855, 0.034)</td>\n",
       "      <td>(0.905, 0.006)</td>\n",
       "      <td>(0.863, 0.03)</td>\n",
       "      <td>(0.05, 0.012)</td>\n",
       "      <td>(0.072, 0.012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set4</th>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>(0.893, 0.008)</td>\n",
       "      <td>(0.845, 0.029)</td>\n",
       "      <td>(0.869, 0.016)</td>\n",
       "      <td>(0.81, 0.053)</td>\n",
       "      <td>(0.893, 0.008)</td>\n",
       "      <td>(0.845, 0.028)</td>\n",
       "      <td>(0.956, 0.005)</td>\n",
       "      <td>(0.911, 0.026)</td>\n",
       "      <td>(0.885, 0.009)</td>\n",
       "      <td>(0.83, 0.033)</td>\n",
       "      <td>(0.901, 0.007)</td>\n",
       "      <td>(0.857, 0.026)</td>\n",
       "      <td>(0.029, 0.009)</td>\n",
       "      <td>(0.045, 0.017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set5</th>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>(0.928, 0.007)</td>\n",
       "      <td>(0.884, 0.026)</td>\n",
       "      <td>(0.933, 0.01)</td>\n",
       "      <td>(0.887, 0.046)</td>\n",
       "      <td>(0.928, 0.007)</td>\n",
       "      <td>(0.885, 0.026)</td>\n",
       "      <td>(0.978, 0.004)</td>\n",
       "      <td>(0.95, 0.016)</td>\n",
       "      <td>(0.928, 0.007)</td>\n",
       "      <td>(0.883, 0.026)</td>\n",
       "      <td>(0.928, 0.007)</td>\n",
       "      <td>(0.884, 0.027)</td>\n",
       "      <td>(0.024, 0.005)</td>\n",
       "      <td>(0.04, 0.009)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train_accuracy   test_accuracy    train_recall  \\\n",
       "set1 100.0 0.00001   (0.886, 0.01)  (0.836, 0.037)   (0.882, 0.02)   \n",
       "set2 100.0 0.00001   (0.92, 0.007)  (0.879, 0.026)  (0.896, 0.012)   \n",
       "set3 100.0 0.00001  (0.903, 0.007)  (0.859, 0.031)  (0.886, 0.015)   \n",
       "set4 100.0 0.00001  (0.893, 0.008)  (0.845, 0.029)  (0.869, 0.016)   \n",
       "set5 100.0 0.00001  (0.928, 0.007)  (0.884, 0.026)   (0.933, 0.01)   \n",
       "\n",
       "                       test_recall       train_mcc        test_mcc  \\\n",
       "set1 100.0 0.00001  (0.841, 0.048)   (0.886, 0.01)  (0.836, 0.038)   \n",
       "set2 100.0 0.00001  (0.856, 0.045)   (0.92, 0.007)   (0.88, 0.026)   \n",
       "set3 100.0 0.00001  (0.836, 0.055)  (0.903, 0.007)  (0.861, 0.031)   \n",
       "set4 100.0 0.00001   (0.81, 0.053)  (0.893, 0.008)  (0.845, 0.028)   \n",
       "set5 100.0 0.00001  (0.887, 0.046)  (0.928, 0.007)  (0.885, 0.026)   \n",
       "\n",
       "                         train_auc        test_auc    train_f1_pos  \\\n",
       "set1 100.0 0.00001  (0.952, 0.006)   (0.904, 0.03)  (0.885, 0.011)   \n",
       "set2 100.0 0.00001  (0.968, 0.004)   (0.938, 0.02)  (0.916, 0.008)   \n",
       "set3 100.0 0.00001  (0.957, 0.006)  (0.918, 0.027)  (0.901, 0.008)   \n",
       "set4 100.0 0.00001  (0.956, 0.005)  (0.911, 0.026)  (0.885, 0.009)   \n",
       "set5 100.0 0.00001  (0.978, 0.004)   (0.95, 0.016)  (0.928, 0.007)   \n",
       "\n",
       "                       test_f1_pos    train_f1_neg     test_f1_neg  \\\n",
       "set1 100.0 0.00001  (0.836, 0.038)  (0.887, 0.009)  (0.835, 0.038)   \n",
       "set2 100.0 0.00001  (0.874, 0.027)  (0.923, 0.007)  (0.883, 0.025)   \n",
       "set3 100.0 0.00001  (0.855, 0.034)  (0.905, 0.006)   (0.863, 0.03)   \n",
       "set4 100.0 0.00001   (0.83, 0.033)  (0.901, 0.007)  (0.857, 0.026)   \n",
       "set5 100.0 0.00001  (0.883, 0.026)  (0.928, 0.007)  (0.884, 0.027)   \n",
       "\n",
       "                          fit_time      score_time  \n",
       "set1 100.0 0.00001  (0.024, 0.009)  (0.037, 0.009)  \n",
       "set2 100.0 0.00001  (0.019, 0.008)  (0.031, 0.006)  \n",
       "set3 100.0 0.00001   (0.05, 0.012)  (0.072, 0.012)  \n",
       "set4 100.0 0.00001  (0.029, 0.009)  (0.045, 0.017)  \n",
       "set5 100.0 0.00001  (0.024, 0.005)   (0.04, 0.009)  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_hyper_results(svm_df, 100, 0.00001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\NMBU\\Miniconda3\\envs\\trying_stuff3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# select model and define the hyperparameter range(s)\n",
    "# logistic regression\n",
    "\n",
    "C = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000] # [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'newton-cholesky', 'sag', 'saga'] # ['newton-cg', 'lbfgs', 'liblinear', 'newton-cholesky', 'sag', 'saga']\n",
    "lr_dict = {}\n",
    "\n",
    "# divide into X and y\n",
    "x = cleaned_df['abstract'].values\n",
    "y = cleaned_df['label'].values\n",
    "\n",
    "# convert X into the features\n",
    "X = vec.transform(x).toarray()\n",
    "res_list = []\n",
    "\n",
    "for c in C:\n",
    "    \n",
    "    for solver in solvers:\n",
    "        \n",
    "        res_dict = {'C':float, 'solver': str, 'metric': {}}\n",
    "        single_res = {'train_accuracy': float, 'test_accuracy': float, 'train_recall': float, 'test_recall': float, 'train_precison': float,\n",
    "                        'test_precison': float,'train_auc': float, 'test_auc': float, 'train_f1_pos':float, 'test_f1_pos': float, 'train_f1_neg': float, 'test_f1_neg': float}\n",
    "\n",
    "        clf = LogisticRegression(C=c, solver=solver, max_iter=300)\n",
    "\n",
    "        # RepeatedStratifiedKfold\n",
    "\n",
    "        kfold = model_selection.RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=1)\n",
    "\n",
    "        scoring = {'accuracy': 'accuracy',\n",
    "                    'recall': 'recall',\n",
    "                    'precison': 'precision',\n",
    "                    'auc': 'roc_auc',\n",
    "                    'f1_pos': 'f1',\n",
    "                    'f1_neg': make_scorer(f1_score, pos_label=0, average='binary')\n",
    "                    }  \n",
    "\n",
    "        cv_results = model_selection.cross_validate(clf, X, y, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "\n",
    "        for metric, values in cv_results.items():\n",
    "            single_res[metric] = (np.round(values.mean(), decimals=3), np.round(values.std(), decimals=3))\n",
    "\n",
    "\n",
    "        res_dict['C'] = c\n",
    "        res_dict['solver'] = solver\n",
    "        res_dict['metric'] = single_res\n",
    "\n",
    "        res_list.append(res_dict)\n",
    "\n",
    "lr_dict['set1'] = res_list\n",
    "    \n",
    "hyper_list = []\n",
    "data_list = []\n",
    "\n",
    "for j, res in lr_dict.items():\n",
    "    \n",
    "    for i in res:\n",
    "        \n",
    "        hyper_list.append((i['C'], i['solver']))\n",
    "\n",
    "        data_list.append(i['metric'])\n",
    "        \n",
    "index = pd.MultiIndex.from_tuples(hyper_list, names=['C', 'solver'])\n",
    "lr_df = pd.DataFrame(data_list, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_accuracy    (0.882, 0.008)\n",
       "test_accuracy     (0.858, 0.032)\n",
       "train_recall      (0.853, 0.012)\n",
       "test_recall       (0.831, 0.055)\n",
       "train_precison    (0.901, 0.009)\n",
       "test_precison     (0.876, 0.048)\n",
       "train_auc         (0.941, 0.005)\n",
       "test_auc          (0.922, 0.026)\n",
       "train_f1_pos      (0.876, 0.009)\n",
       "test_f1_pos       (0.851, 0.035)\n",
       "train_f1_neg      (0.887, 0.008)\n",
       "test_f1_neg       (0.864, 0.031)\n",
       "fit_time          (0.018, 0.003)\n",
       "score_time        (0.011, 0.002)\n",
       "Name: (0.001, lbfgs), dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_df.loc[ (0.001, 'lbfgs'),:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model and define the hyperparameter range(s)\n",
    "# random forest\n",
    "\n",
    "N = [1,2,3,4,5,6,7,8,9] + [100] + [1000] + [10000]\n",
    "criterions = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "rf_dict = {}\n",
    "    \n",
    "# divide into X and y\n",
    "x = cleaned_df['abstract'].values\n",
    "y = cleaned_df['label'].values\n",
    "\n",
    "# convert X into the features\n",
    "X = vec.transform(x).toarray()\n",
    "res_list = []\n",
    "\n",
    "for n in N:\n",
    "    \n",
    "    for crit in criterions:\n",
    "        \n",
    "        res_dict = {'C':float, 'criterion': str, 'metric': {}}\n",
    "        single_res = {'train_accuracy': float, 'test_accuracy': float, 'train_recall': float, 'test_recall': float, 'train_precison': float,\n",
    "                        'test_precison': float,'train_auc': float, 'test_auc': float, 'train_f1_pos':float, 'test_f1_pos': float, 'train_f1_neg': float, 'test_f1_neg': float}\n",
    "\n",
    "        \n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=n, criterion=crit)\n",
    "\n",
    "        # RepeatedStratifiedKfold\n",
    "\n",
    "        kfold = model_selection.RepeatedStratifiedKFold(n_splits=4, n_repeats=5, random_state=1)\n",
    "\n",
    "        scoring = {'accuracy': 'accuracy',\n",
    "                    'recall': 'recall',\n",
    "                    'precison': 'precision',\n",
    "                    'auc': 'roc_auc',\n",
    "                    'f1_pos': 'f1',\n",
    "                    'f1_neg': make_scorer(f1_score, pos_label=0, average='binary')\n",
    "                    }  \n",
    "\n",
    "        cv_results = model_selection.cross_validate(clf, X, y, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "\n",
    "        for metric, values in cv_results.items():\n",
    "            single_res[metric] = (np.round(values.mean(), decimals=3), np.round(values.std(), decimals=3))\n",
    "\n",
    "\n",
    "        res_dict['N'] = n\n",
    "        res_dict['criterion'] = crit\n",
    "        res_dict['metric'] = single_res\n",
    "\n",
    "        res_list.append(res_dict)\n",
    "\n",
    "rf_dict['set1'] = res_list\n",
    "    \n",
    "hyper_list = []\n",
    "data_list = []\n",
    "\n",
    "for j, res in rf_dict.items():\n",
    "    \n",
    "    for i in res:\n",
    "        \n",
    "        hyper_list.append((i['N'], i['criterion']))\n",
    "\n",
    "        data_list.append(i['metric'])\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(hyper_list, names=['n-estimators', 'solver'])\n",
    "rf_df = pd.DataFrame(data_list, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_accuracy         (1.0, 0.0)\n",
       "test_accuracy      (0.879, 0.031)\n",
       "train_recall           (1.0, 0.0)\n",
       "test_recall        (0.877, 0.047)\n",
       "train_precison         (1.0, 0.0)\n",
       "test_precison       (0.878, 0.04)\n",
       "train_auc              (1.0, 0.0)\n",
       "test_auc            (0.945, 0.02)\n",
       "train_f1_pos           (1.0, 0.0)\n",
       "test_f1_pos        (0.876, 0.033)\n",
       "train_f1_neg           (1.0, 0.0)\n",
       "test_f1_neg        (0.881, 0.031)\n",
       "fit_time          (27.141, 0.301)\n",
       "score_time         (2.577, 0.213)\n",
       "Name: (10000, entropy), dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_df.loc[(10000, 'entropy')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Shallow neural net\n",
    "\n",
    "### 3.3.1 Tuning with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<250x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15934 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.transform(cleaned_df['abstract'].sample(frac=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_fold = []\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    t = train.loc[train_index, :]\n",
    "    v = train.loc[test_index, :]\n",
    "\n",
    "    five_fold.append((t,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-09-10 22:44:01,394]\u001b[0m A new study created in memory with name: BoW-features\u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:44:39,268]\u001b[0m Trial 0 finished with values: [3.7045527935028075, 0.5954364538192749, 0.5999999940395355, 0.9714285731315613] and parameters: {'n_hidden': 3, 'n_units': 42, 'kernel_regularizer': 3.252320601075059e-05, 'learning_rate': 0.4356979856277655, 'dropout': 0.21382376483484197, 'threshold': 0.37404947489402296}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:45:22,882]\u001b[0m Trial 1 finished with values: [23.062834167480467, 0.5641550183296203, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 8, 'n_units': 9, 'kernel_regularizer': 0.04752892339894869, 'learning_rate': 1.4988822340920365e-05, 'dropout': 0.048525896101412724, 'threshold': 0.27265874826006264}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:46:04,789]\u001b[0m Trial 2 finished with values: [0.9944956064224243, 0.8584943056106568, 0.7419999957084655, 0.8596825480461121] and parameters: {'n_hidden': 7, 'n_units': 6, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.0013414232848841775, 'dropout': 0.6061345542965496, 'threshold': 0.27938857146053786}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:46:51,603]\u001b[0m Trial 3 finished with values: [7.108174133300781, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 7, 'n_units': 36, 'kernel_regularizer': 0.005125020260650322, 'learning_rate': 0.23787299362350237, 'dropout': 0.8550883431327796, 'threshold': 0.13450895973628452}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:47:28,793]\u001b[0m Trial 4 finished with values: [0.4125930964946747, 0.9089539885520935, 0.806000006198883, 0.8347406029701233] and parameters: {'n_hidden': 1, 'n_units': 33, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.1697866286480649, 'threshold': 0.48870052508813955}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:48:12,874]\u001b[0m Trial 5 finished with values: [2.431282949447632, 0.7877654194831848, 0.6199999988079071, 0.39186834916472435] and parameters: {'n_hidden': 5, 'n_units': 64, 'kernel_regularizer': 0.0004416315616331735, 'learning_rate': 1.0619818967848172e-05, 'dropout': 0.35889713741090246, 'threshold': 0.5254734187445315}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:48:58,933]\u001b[0m Trial 6 finished with values: [1431.3957397460938, 0.502173912525177, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 6, 'n_units': 49, 'kernel_regularizer': 0.08691728185213228, 'learning_rate': 0.5300889580966471, 'dropout': 0.41146991286407414, 'threshold': 0.8674801877746112}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:49:38,068]\u001b[0m Trial 7 finished with values: [0.7951945424079895, 0.9043636918067932, 0.8439999938011169, 0.8787260293960572] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:50:17,070]\u001b[0m Trial 8 finished with values: [0.8880736470222473, 0.9148423314094544, 0.8420000076293945, 0.7914808034896851] and parameters: {'n_hidden': 3, 'n_units': 36, 'kernel_regularizer': 0.0007793010643204281, 'learning_rate': 0.0003795051194324455, 'dropout': 0.7010918993652716, 'threshold': 0.8252501260451032}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:50:58,164]\u001b[0m Trial 9 finished with values: [4.138941812515259, 0.7198055386543274, 0.6759999930858612, 0.4314862847328186] and parameters: {'n_hidden': 4, 'n_units': 35, 'kernel_regularizer': 0.01053292796946288, 'learning_rate': 0.06403568421629276, 'dropout': 0.09122805429551042, 'threshold': 0.8847940712676493}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:51:36,791]\u001b[0m Trial 10 finished with values: [0.8728283286094666, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 4, 'n_units': 11, 'kernel_regularizer': 0.022186658522395757, 'learning_rate': 0.005679615302586204, 'dropout': 0.8743938136435966, 'threshold': 0.8166827909823133}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:52:27,608]\u001b[0m Trial 11 finished with values: [0.5997413456439972, 0.908999228477478, 0.8399999976158142, 0.7996674299240112] and parameters: {'n_hidden': 9, 'n_units': 54, 'kernel_regularizer': 1.8006114411320668e-05, 'learning_rate': 6.444708842918975e-05, 'dropout': 0.36324986218858296, 'threshold': 0.7581930049210841}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:53:14,717]\u001b[0m Trial 12 finished with values: [4.1070411682128904, 0.8223453164100647, 0.8100000023841858, 0.8322696447372436] and parameters: {'n_hidden': 6, 'n_units': 14, 'kernel_regularizer': 8.109315860187576e-05, 'learning_rate': 0.020071355498683455, 'dropout': 0.5173607547941139, 'threshold': 0.35187361869838785}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:53:56,501]\u001b[0m Trial 13 finished with values: [3.158517599105835, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 5, 'n_units': 25, 'kernel_regularizer': 0.008621466060909468, 'learning_rate': 0.08391900760724622, 'dropout': 0.5499841382958144, 'threshold': 0.44919456440413785}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:54:38,342]\u001b[0m Trial 14 finished with values: [1.1129023075103759, 0.8719367504119873, 0.8099999904632569, 0.8433106660842895] and parameters: {'n_hidden': 6, 'n_units': 23, 'kernel_regularizer': 0.0019117718670064964, 'learning_rate': 0.0012602416730026099, 'dropout': 0.21633598320305647, 'threshold': 0.3893046333814376}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:55:21,878]\u001b[0m Trial 15 finished with values: [1.005884051322937, 0.5593397736549377, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 7, 'n_units': 2, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.06347803812641022, 'dropout': 0.16323807071529978, 'threshold': 0.1394364965085842}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:56:01,450]\u001b[0m Trial 16 finished with values: [1.5486692428588866, 0.8476960182189941, 0.7900000095367432, 0.8492159843444824] and parameters: {'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 0.00023198013313174801, 'learning_rate': 0.009763246058742354, 'dropout': 0.11174948366441749, 'threshold': 0.300791908635915}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:56:44,032]\u001b[0m Trial 17 finished with values: [220.3354736328125, 0.5, 0.5300000011920929, 0.2] and parameters: {'n_hidden': 6, 'n_units': 14, 'kernel_regularizer': 0.2008611574966681, 'learning_rate': 0.6775041787779892, 'dropout': 0.27368177112523867, 'threshold': 0.501873524097572}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:57:32,736]\u001b[0m Trial 18 finished with values: [1.42855361700058, 0.8760056734085083, 0.8239999890327454, 0.8494262337684632] and parameters: {'n_hidden': 8, 'n_units': 39, 'kernel_regularizer': 0.00018875506541698138, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.24513193648010934}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:58:20,737]\u001b[0m Trial 19 finished with values: [54.97541427612305, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 64, 'kernel_regularizer': 0.2088821347133472, 'learning_rate': 0.021296428950781383, 'dropout': 0.6072104292992969, 'threshold': 0.36807372516731574}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:59:05,964]\u001b[0m Trial 20 finished with values: [4.96717939376831, 0.7090253472328186, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 7, 'n_units': 24, 'kernel_regularizer': 0.003098807640434317, 'learning_rate': 1.942350227907491e-05, 'dropout': 0.7444342348199258, 'threshold': 0.6488403821480769}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 22:59:44,823]\u001b[0m Trial 21 finished with values: [0.9647250890731811, 0.8848425388336182, 0.8319999933242798, 0.8636157512664795] and parameters: {'n_hidden': 3, 'n_units': 22, 'kernel_regularizer': 0.000918416956060503, 'learning_rate': 0.00378997837281349, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:00:29,177]\u001b[0m Trial 22 finished with values: [1.151072585582733, 0.8668260216712952, 0.8339999914169312, 0.8324249267578125] and parameters: {'n_hidden': 6, 'n_units': 37, 'kernel_regularizer': 0.0010579205679920412, 'learning_rate': 0.0049473859340445365, 'dropout': 0.2862399358316623, 'threshold': 0.8171312422835556}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:01:12,129]\u001b[0m Trial 23 finished with values: [0.7620733499526977, 0.5628686904907226, 0.55799999833107, 0.1288888931274414] and parameters: {'n_hidden': 7, 'n_units': 2, 'kernel_regularizer': 9.293850500013539e-05, 'learning_rate': 0.025379351434645, 'dropout': 0.3188170831736297, 'threshold': 0.8619292156439865}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:01:51,503]\u001b[0m Trial 24 finished with values: [0.8772236585617066, 0.8821631073951721, 0.8019999980926513, 0.7029423475265503] and parameters: {'n_hidden': 2, 'n_units': 22, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 0.007785363959396076, 'dropout': 0.8362620725804383, 'threshold': 0.8728591551405842}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:02:34,783]\u001b[0m Trial 25 finished with values: [22.129780197143553, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 5, 'n_units': 33, 'kernel_regularizer': 0.0006863451263142699, 'learning_rate': 0.5743365897491732, 'dropout': 0.20902702870023096, 'threshold': 0.47449828546943207}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:03:13,940]\u001b[0m Trial 26 finished with values: [3.606445550918579, 0.7883756041526795, 0.7700000047683716, 0.6445860028266907] and parameters: {'n_hidden': 3, 'n_units': 40, 'kernel_regularizer': 0.0024977229168431335, 'learning_rate': 0.1838154555851199, 'dropout': 0.0701658967497424, 'threshold': 0.6253315162626382}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:04:00,232]\u001b[0m Trial 27 finished with values: [3419.209228515625, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 48, 'kernel_regularizer': 0.025591167243364892, 'learning_rate': 0.9634712346819564, 'dropout': 0.18960251226025235, 'threshold': 0.31665717730358856}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:04:44,567]\u001b[0m Trial 28 finished with values: [2.0172148942947388, 0.766096568107605, 0.7639999985694885, 0.8523039937019348] and parameters: {'n_hidden': 6, 'n_units': 20, 'kernel_regularizer': 0.001412413222444168, 'learning_rate': 0.12425482536238662, 'dropout': 0.31650942892978745, 'threshold': 0.43263615757197926}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:05:32,120]\u001b[0m Trial 29 finished with values: [0.873017418384552, 0.6704458236694336, 0.5100000083446503, 0.9777777791023254] and parameters: {'n_hidden': 6, 'n_units': 50, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 1.1875114388657349e-05, 'dropout': 0.8222956921294372, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:06:17,646]\u001b[0m Trial 30 finished with values: [4.4375307083129885, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 5, 'n_units': 50, 'kernel_regularizer': 0.055797915215164494, 'learning_rate': 0.008453440294222273, 'dropout': 0.42856543244939366, 'threshold': 0.7113145004826874}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:07:02,382]\u001b[0m Trial 31 finished with values: [50.338707733154294, 0.5999812006950378, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 5, 'n_units': 44, 'kernel_regularizer': 0.03160996808956894, 'learning_rate': 8.446138571374139e-05, 'dropout': 0.8630165121896143, 'threshold': 0.28484961135147746}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:07:51,379]\u001b[0m Trial 32 finished with values: [1.923224723339081, 0.8513466715812683, 0.8279999852180481, 0.8570713996887207] and parameters: {'n_hidden': 9, 'n_units': 32, 'kernel_regularizer': 1.8148712421091602e-05, 'learning_rate': 0.005952816600700391, 'dropout': 0.1541069665813928, 'threshold': 0.52263525360727}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:08:41,855]\u001b[0m Trial 33 finished with values: [0.8901241540908813, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 10, 'n_units': 28, 'kernel_regularizer': 0.00013596310778502045, 'learning_rate': 0.11924010297295179, 'dropout': 0.6560141509711666, 'threshold': 0.8202239364592316}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:09:31,676]\u001b[0m Trial 34 finished with values: [20.46909370422363, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 0.0004207977480915038, 'learning_rate': 0.322415907429002, 'dropout': 0.8773890275264329, 'threshold': 0.7339310635266302}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:10:21,578]\u001b[0m Trial 35 finished with values: [13.573879432678222, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 9, 'n_units': 38, 'kernel_regularizer': 0.025036536983171216, 'learning_rate': 0.00041108121616871145, 'dropout': 0.8225587625119328, 'threshold': 0.16828180575942753}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:11:09,441]\u001b[0m Trial 36 finished with values: [2.5617151498794555, 0.8433434247970581, 0.8359999895095825, 0.8772088170051575] and parameters: {'n_hidden': 9, 'n_units': 16, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.7860688156703391, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:11:54,568]\u001b[0m Trial 37 finished with values: [2.0729789257049562, 0.8491316318511963, 0.8139999866485595, 0.860575819015503] and parameters: {'n_hidden': 7, 'n_units': 18, 'kernel_regularizer': 0.00016585394479730057, 'learning_rate': 0.012576086124314958, 'dropout': 0.8077040619685475, 'threshold': 0.6021964792751174}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:12:32,590]\u001b[0m Trial 38 finished with values: [0.7858430027961731, 0.8970683097839356, 0.8299999952316284, 0.8743489384651184] and parameters: {'n_hidden': 1, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.0035197135737012686, 'dropout': 0.6376447344086704, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:13:19,162]\u001b[0m Trial 39 finished with values: [11.412005615234374, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 46, 'kernel_regularizer': 0.5540476933463919, 'learning_rate': 0.0027041786234411743, 'dropout': 0.48726918436401034, 'threshold': 0.38558963176039396}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:14:08,286]\u001b[0m Trial 40 finished with values: [0.9420882344245911, 0.7976283431053162, 0.5459999978542328, 0.06909090876579285] and parameters: {'n_hidden': 10, 'n_units': 13, 'kernel_regularizer': 0.0004081577143659623, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.10114145915245089, 'threshold': 0.5999518187639323}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:14:53,991]\u001b[0m Trial 41 finished with values: [10.85358772277832, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 7, 'n_units': 22, 'kernel_regularizer': 0.07065315324578661, 'learning_rate': 0.0006018048672022595, 'dropout': 0.5779933306490018, 'threshold': 0.6547177756610225}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:15:42,130]\u001b[0m Trial 42 finished with values: [933.0690307617188, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 8, 'n_units': 47, 'kernel_regularizer': 0.383163095770306, 'learning_rate': 0.3417854867591764, 'dropout': 0.4254477581054053, 'threshold': 0.23753211686694353}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:16:23,907]\u001b[0m Trial 43 finished with values: [0.685663366317749, 0.9199808359146118, 0.8420000195503234, 0.7594310522079468] and parameters: {'n_hidden': 3, 'n_units': 44, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.00020655775452336643, 'dropout': 0.3395203701483684, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:17:06,519]\u001b[0m Trial 44 finished with values: [1.5527375936508179, 0.8525785684585572, 0.8239999890327454, 0.8446945667266845] and parameters: {'n_hidden': 5, 'n_units': 16, 'kernel_regularizer': 0.00022030814846372472, 'learning_rate': 0.005368887024829273, 'dropout': 0.26449892004430114, 'threshold': 0.8895057565427082}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:17:48,909]\u001b[0m Trial 45 finished with values: [1.6334412574768067, 0.8895434737205505, 0.8220000028610229, 0.7851508378982544] and parameters: {'n_hidden': 3, 'n_units': 45, 'kernel_regularizer': 0.018455732471100276, 'learning_rate': 0.007543213686442456, 'dropout': 0.7646124647190425, 'threshold': 0.7610837794265474}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:18:28,387]\u001b[0m Trial 46 finished with values: [118.1608169555664, 0.5184983491897583, 0.4880000054836273, 0.9782312989234925] and parameters: {'n_hidden': 2, 'n_units': 41, 'kernel_regularizer': 0.07865596846268307, 'learning_rate': 2.3707141441912732e-05, 'dropout': 0.08838592982718305, 'threshold': 0.27429124215582346}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:19:07,585]\u001b[0m Trial 47 finished with values: [0.5475440442562103, 0.9232970118522644, 0.85, 0.8014498710632324] and parameters: {'n_hidden': 1, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0005834382440750414, 'dropout': 0.48787589688314237, 'threshold': 0.7622802703643394}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:19:54,829]\u001b[0m Trial 48 finished with values: [5.070920610427857, 0.8097846388816834, 0.8, 0.8308458805084229] and parameters: {'n_hidden': 9, 'n_units': 16, 'kernel_regularizer': 5.611422542025095e-05, 'learning_rate': 0.06799047081413942, 'dropout': 0.5039281376641517, 'threshold': 0.506262197372691}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:20:42,753]\u001b[0m Trial 49 finished with values: [6.278939628601075, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 8, 'n_units': 28, 'kernel_regularizer': 0.20525919612633622, 'learning_rate': 0.000912480727181795, 'dropout': 0.622148127162847, 'threshold': 0.3683841096343935}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:21:28,914]\u001b[0m Trial 50 finished with values: [1.8074174880981446, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 46, 'kernel_regularizer': 0.05778245675226843, 'learning_rate': 0.0027041786234411743, 'dropout': 0.48726918436401034, 'threshold': 0.38558963176039396}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:22:15,906]\u001b[0m Trial 51 finished with values: [1.0734293103218078, 0.8893179416656494, 0.8140000104904175, 0.8607806086540222] and parameters: {'n_hidden': 6, 'n_units': 39, 'kernel_regularizer': 0.001412413222444168, 'learning_rate': 0.0007582681194411686, 'dropout': 0.31650942892978745, 'threshold': 0.24513193648010934}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:23:01,195]\u001b[0m Trial 52 finished with values: [0.8310856223106384, 0.8935249447822571, 0.8279999971389771, 0.7743242025375366] and parameters: {'n_hidden': 5, 'n_units': 18, 'kernel_regularizer': 0.00022030814846372472, 'learning_rate': 0.00042063660106749336, 'dropout': 0.26449892004430114, 'threshold': 0.8895057565427082}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:23:52,189]\u001b[0m Trial 53 finished with values: [0.6550341725349427, 0.8948416590690613, 0.5240000009536743, 0.9959183692932129] and parameters: {'n_hidden': 10, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.6376447344086704, 'threshold': 0.2292333004984978}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:24:30,137]\u001b[0m Trial 54 finished with values: [0.5365613222122192, 0.9184743523597717, 0.8440000057220459, 0.7708527445793152] and parameters: {'n_hidden': 1, 'n_units': 37, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0006521686023472134, 'dropout': 0.2862399358316623, 'threshold': 0.8171312422835556}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:25:16,491]\u001b[0m Trial 55 finished with values: [11.805007827281951, 0.8029743432998657, 0.7919999957084656, 0.8362674355506897] and parameters: {'n_hidden': 8, 'n_units': 16, 'kernel_regularizer': 0.00018875506541698138, 'learning_rate': 0.06799047081413942, 'dropout': 0.5039281376641517, 'threshold': 0.506262197372691}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:26:04,982]\u001b[0m Trial 56 finished with values: [0.7097243309020996, 0.6112584114074707, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 9, 'n_units': 16, 'kernel_regularizer': 1.8006114411320668e-05, 'learning_rate': 1.407676211616156e-05, 'dropout': 0.30926352361424064, 'threshold': 0.7581930049210841}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:26:42,536]\u001b[0m Trial 57 finished with values: [2.676980400085449, 0.7936557531356812, 0.7160000085830689, 0.9707634091377259] and parameters: {'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.5499841382958144, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:27:32,419]\u001b[0m Trial 58 finished with values: [4.2997959613800045, 0.8316033124923706, 0.8199999928474426, 0.854831314086914] and parameters: {'n_hidden': 9, 'n_units': 21, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 0.0198841538851292, 'dropout': 0.26449892004430114, 'threshold': 0.38046412440491495}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:28:14,751]\u001b[0m Trial 59 finished with values: [81.52826385498047, 0.5390899240970611, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 4, 'n_units': 35, 'kernel_regularizer': 0.04752892339894869, 'learning_rate': 1.4988822340920365e-05, 'dropout': 0.048525896101412724, 'threshold': 0.27265874826006264}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:28:54,128]\u001b[0m Trial 60 finished with values: [0.8167295336723328, 0.9130750894546509, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 2, 'n_units': 11, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 0.00039626071240343367, 'dropout': 0.8362620725804383, 'threshold': 0.10873515186137227}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:29:35,052]\u001b[0m Trial 61 finished with values: [2.867680215835571, 0.8607740283012391, 0.8240000009536743, 0.8443551182746887] and parameters: {'n_hidden': 3, 'n_units': 45, 'kernel_regularizer': 0.0019117718670064964, 'learning_rate': 0.13216459374881756, 'dropout': 0.21633598320305647, 'threshold': 0.7610837794265474}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:30:27,334]\u001b[0m Trial 62 finished with values: [1.1060791969299317, 0.5063838243484498, 0.4939999938011169, 0.2618181824684143] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.06799047081413942, 'dropout': 0.3395203701483684, 'threshold': 0.506262197372691}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:31:21,467]\u001b[0m Trial 63 finished with values: [6.043911266326904, 0.827033531665802, 0.821999990940094, 0.8375372767448426] and parameters: {'n_hidden': 9, 'n_units': 50, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.7860688156703391, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:32:09,142]\u001b[0m Trial 64 finished with values: [8.783171844482421, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 38, 'kernel_regularizer': 0.025036536983171216, 'learning_rate': 0.00041108121616871145, 'dropout': 0.8225587625119328, 'threshold': 0.16828180575942753}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:32:56,184]\u001b[0m Trial 65 finished with values: [1.104770255088806, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 8, 'n_units': 11, 'kernel_regularizer': 0.04752892339894869, 'learning_rate': 0.005679615302586204, 'dropout': 0.27946286276211874, 'threshold': 0.27265874826006264}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:33:39,934]\u001b[0m Trial 66 finished with values: [0.8519971132278442, 0.4720432281494141, 0.5039999961853028, 0.25026455521583557] and parameters: {'n_hidden': 5, 'n_units': 18, 'kernel_regularizer': 0.00016585394479730057, 'learning_rate': 1.0619818967848172e-05, 'dropout': 0.8077040619685475, 'threshold': 0.5254734187445315}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:34:29,360]\u001b[0m Trial 67 finished with values: [2.183260273933411, 0.8477096199989319, 0.8080000042915344, 0.8963375210762023] and parameters: {'n_hidden': 9, 'n_units': 16, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:35:16,911]\u001b[0m Trial 68 finished with values: [2.077702760696411, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 46, 'kernel_regularizer': 0.20525919612633622, 'learning_rate': 0.000912480727181795, 'dropout': 0.19574656681834715, 'threshold': 0.3683841096343935}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:36:03,405]\u001b[0m Trial 69 finished with values: [4.950693225860595, 0.5821903347969055, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 7, 'n_units': 24, 'kernel_regularizer': 0.003098807640434317, 'learning_rate': 1.942350227907491e-05, 'dropout': 0.7444342348199258, 'threshold': 0.6488403821480769}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:36:52,785]\u001b[0m Trial 70 finished with values: [530.9208251953125, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 8, 'n_units': 34, 'kernel_regularizer': 0.383163095770306, 'learning_rate': 0.3417854867591764, 'dropout': 0.20896209008607705, 'threshold': 0.30239565388321293}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:37:40,896]\u001b[0m Trial 71 finished with values: [1.6324713706970215, 0.8503427386283875, 0.8080000042915344, 0.8283529162406922] and parameters: {'n_hidden': 6, 'n_units': 40, 'kernel_regularizer': 0.0024977229168431335, 'learning_rate': 0.020071355498683455, 'dropout': 0.5173607547941139, 'threshold': 0.35187361869838785}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:38:31,413]\u001b[0m Trial 72 finished with values: [1.2330559611320495, 0.8658406138420105, 0.822000014781952, 0.8490551829338073] and parameters: {'n_hidden': 7, 'n_units': 22, 'kernel_regularizer': 0.000918416956060503, 'learning_rate': 0.00378997837281349, 'dropout': 0.8077040619685475, 'threshold': 0.25478675673102114}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:39:12,058]\u001b[0m Trial 73 finished with values: [1.7488561868667603, 0.8555118441581726, 0.8119999885559082, 0.8455947160720825] and parameters: {'n_hidden': 3, 'n_units': 14, 'kernel_regularizer': 1.8006114411320668e-05, 'learning_rate': 0.009763246058742354, 'dropout': 0.11174948366441749, 'threshold': 0.7581930049210841}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:39:57,324]\u001b[0m Trial 74 finished with values: [0.7229814767837525, 0.8963444709777832, 0.8399999976158142, 0.8388291120529174] and parameters: {'n_hidden': 7, 'n_units': 6, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0013414232848841775, 'dropout': 0.6061345542965496, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:40:49,181]\u001b[0m Trial 75 finished with values: [1.3287604093551635, 0.9138515591621399, 0.8419999957084656, 0.8087610840797425] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 0.00032005478454991034, 'learning_rate': 8.446138571374139e-05, 'dropout': 0.8630165121896143, 'threshold': 0.52263525360727}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:41:33,839]\u001b[0m Trial 76 finished with values: [1.8553133368492127, 0.8496733188629151, 0.8079999923706055, 0.8695416808128357] and parameters: {'n_hidden': 6, 'n_units': 16, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.8222956921294372, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:42:15,482]\u001b[0m Trial 77 finished with values: [0.7465394139289856, 0.5070725440979004, 0.49999999403953554, 0.0037037037312984467] and parameters: {'n_hidden': 2, 'n_units': 2, 'kernel_regularizer': 0.0004416315616331735, 'learning_rate': 1.0619818967848172e-05, 'dropout': 0.35889713741090246, 'threshold': 0.5254734187445315}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:42:55,842]\u001b[0m Trial 78 finished with values: [1.3832631826400756, 0.8541081190109253, 0.8020000100135803, 0.8525499820709228] and parameters: {'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 0.00023198013313174801, 'learning_rate': 0.009763246058742354, 'dropout': 0.11174948366441749, 'threshold': 0.31665717730358856}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:43:39,615]\u001b[0m Trial 79 finished with values: [1.7220921277999879, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 5, 'n_units': 11, 'kernel_regularizer': 0.008621466060909468, 'learning_rate': 0.08391900760724622, 'dropout': 0.8743938136435966, 'threshold': 0.8166827909823133}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:44:30,186]\u001b[0m Trial 80 finished with values: [2.667480802536011, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 64, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 0.021296428950781383, 'dropout': 0.6072104292992969, 'threshold': 0.36807372516731574}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:45:12,612]\u001b[0m Trial 81 finished with values: [10.908636665344238, 0.8201926112174988, 0.7900000095367432, 0.8291307687759399] and parameters: {'n_hidden': 4, 'n_units': 25, 'kernel_regularizer': 5.0989648131157565e-05, 'learning_rate': 0.08391900760724622, 'dropout': 0.5499841382958144, 'threshold': 0.8252501260451032}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:45:57,569]\u001b[0m Trial 82 finished with values: [6.14049072265625, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 20, 'kernel_regularizer': 0.015040262427345498, 'learning_rate': 0.12425482536238662, 'dropout': 0.31650942892978745, 'threshold': 0.43263615757197926}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:46:41,793]\u001b[0m Trial 83 finished with values: [9.750508880615234, 0.5, 0.5060000002384186, 0.2] and parameters: {'n_hidden': 6, 'n_units': 14, 'kernel_regularizer': 0.0010579205679920412, 'learning_rate': 0.6775041787779892, 'dropout': 0.2862399358316623, 'threshold': 0.501873524097572}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:47:22,784]\u001b[0m Trial 84 finished with values: [6.717262172698975, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 3, 'n_units': 4, 'kernel_regularizer': 0.7718482864373142, 'learning_rate': 0.0007970799285309584, 'dropout': 0.09122805429551042, 'threshold': 0.8847940712676493}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:48:12,471]\u001b[0m Trial 85 finished with values: [5.765780782699585, 0.8198513984680176, 0.8139999985694886, 0.837725555896759] and parameters: {'n_hidden': 8, 'n_units': 39, 'kernel_regularizer': 1.466543809513448e-05, 'learning_rate': 0.007543213686442456, 'dropout': 0.7646124647190425, 'threshold': 0.7610837794265474}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:49:04,092]\u001b[0m Trial 86 finished with values: [1.4554840087890626, 0.8714451909065246, 0.8359999895095825, 0.812731397151947] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 7.059034212989332e-05, 'learning_rate': 0.00020655775452336643, 'dropout': 0.5039281376641517, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:49:52,493]\u001b[0m Trial 87 finished with values: [3.2229043364524843, 0.6355222940444947, 0.5780000030994416, 0.16734694242477416] and parameters: {'n_hidden': 7, 'n_units': 22, 'kernel_regularizer': 0.00013596310778502045, 'learning_rate': 0.11924010297295179, 'dropout': 0.5779933306490018, 'threshold': 0.8202239364592316}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:50:31,942]\u001b[0m Trial 88 finished with values: [3.7807660579681395, 0.9080213665962219, 0.7760000109672547, 0.6265086233615875] and parameters: {'n_hidden': 1, 'n_units': 35, 'kernel_regularizer': 0.01053292796946288, 'learning_rate': 0.06403568421629276, 'dropout': 0.36324986218858296, 'threshold': 0.7581930049210841}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:51:19,245]\u001b[0m Trial 89 finished with values: [11.839859771728516, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 8, 'n_units': 17, 'kernel_regularizer': 0.20525919612633622, 'learning_rate': 0.000912480727181795, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:52:13,627]\u001b[0m Trial 90 finished with values: [1.9683311223983764, 0.9083575010299683, 0.7620000064373016, 0.6267573833465576] and parameters: {'n_hidden': 10, 'n_units': 48, 'kernel_regularizer': 0.0004081577143659623, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.8601891259731952, 'threshold': 0.5999518187639323}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:52:59,064]\u001b[0m Trial 91 finished with values: [7.985191607475281, 0.8241465330123902, 0.771999990940094, 0.7047715187072754] and parameters: {'n_hidden': 3, 'n_units': 36, 'kernel_regularizer': 0.00013596310778502045, 'learning_rate': 0.11924010297295179, 'dropout': 0.7010918993652716, 'threshold': 0.6590883285243608}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:53:46,179]\u001b[0m Trial 92 finished with values: [3.7879095554351805, 0.5017241358757019, 0.49200000762939455, 1.0] and parameters: {'n_hidden': 5, 'n_units': 33, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.5743365897491732, 'dropout': 0.20902702870023096, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:54:33,676]\u001b[0m Trial 93 finished with values: [95.65381317138672, 0.4559819161891937, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 7, 'n_units': 22, 'kernel_regularizer': 0.07865596846268307, 'learning_rate': 2.3707141441912732e-05, 'dropout': 0.5779933306490018, 'threshold': 0.6547177756610225}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:55:12,073]\u001b[0m Trial 94 finished with values: [0.834935462474823, 0.9121012926101685, 0.8340000033378601, 0.7514972925186157] and parameters: {'n_hidden': 1, 'n_units': 33, 'kernel_regularizer': 0.0004081577143659623, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.10114145915245089, 'threshold': 0.5999518187639323}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:55:54,234]\u001b[0m Trial 95 finished with values: [0.8865840435028076, 0.9080583333969117, 0.8299999952316284, 0.8882209897041321] and parameters: {'n_hidden': 1, 'n_units': 64, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.021296428950781383, 'dropout': 0.8662788901341136, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:56:37,716]\u001b[0m Trial 96 finished with values: [15.931554222106934, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 5, 'n_units': 25, 'kernel_regularizer': 0.05102519708024553, 'learning_rate': 0.08391900760724622, 'dropout': 0.5499841382958144, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:57:24,537]\u001b[0m Trial 97 finished with values: [0.9474589407444001, 0.8744781732559204, 0.7700000047683716, 0.9125664830207825] and parameters: {'n_hidden': 7, 'n_units': 6, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.0013414232848841775, 'dropout': 0.6061345542965496, 'threshold': 0.27938857146053786}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:58:05,893]\u001b[0m Trial 98 finished with values: [2.360753583908081, 0.8509045839309692, 0.8159999966621398, 0.857107138633728] and parameters: {'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 2.0678394943630145e-05, 'learning_rate': 0.009763246058742354, 'dropout': 0.48726918436401034, 'threshold': 0.300791908635915}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:58:55,639]\u001b[0m Trial 99 finished with values: [2.1973902583122253, 0.6825390100479126, 0.64200000166893, 0.8562963008880615] and parameters: {'n_hidden': 9, 'n_units': 25, 'kernel_regularizer': 1.8148712421091602e-05, 'learning_rate': 0.08391900760724622, 'dropout': 0.5499841382958144, 'threshold': 0.44919456440413785}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-10 23:59:43,265]\u001b[0m Trial 100 finished with values: [1.2911315441131592, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 8, 'n_units': 9, 'kernel_regularizer': 0.04752892339894869, 'learning_rate': 0.009763246058742354, 'dropout': 0.048525896101412724, 'threshold': 0.8136390702330866}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:00:22,600]\u001b[0m Trial 101 finished with values: [0.47009494304656985, 0.9075527787208557, 0.7359999895095826, 0.49238097071647646] and parameters: {'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 1.8006114411320668e-05, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.30926352361424064, 'threshold': 0.7581930049210841}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:01:12,372]\u001b[0m Trial 102 finished with values: [1.4101351022720336, 0.6810372948646546, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 6, 'n_units': 50, 'kernel_regularizer': 0.00022030814846372472, 'learning_rate': 1.1875114388657349e-05, 'dropout': 0.8222956921294372, 'threshold': 0.8895057565427082}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:02:01,674]\u001b[0m Trial 103 finished with values: [4.4465094089508055, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 5, 'n_units': 39, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.5743365897491732, 'dropout': 0.20902702870023096, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:02:40,719]\u001b[0m Trial 104 finished with values: [0.4775063216686249, 0.9200471878051758, 0.8299999952316284, 0.880948257446289] and parameters: {'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.3395203701483684, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:03:28,902]\u001b[0m Trial 105 finished with values: [2.1959473848342896, 0.8133375167846679, 0.7899999976158142, 0.8569408416748047] and parameters: {'n_hidden': 8, 'n_units': 18, 'kernel_regularizer': 0.00016585394479730057, 'learning_rate': 0.005368887024829273, 'dropout': 0.26449892004430114, 'threshold': 0.6021964792751174}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:04:20,351]\u001b[0m Trial 106 finished with values: [4.311386442184448, 0.8347828149795532, 0.8340000152587891, 0.8718092441558838] and parameters: {'n_hidden': 9, 'n_units': 33, 'kernel_regularizer': 1.8148712421091602e-05, 'learning_rate': 0.005952816600700391, 'dropout': 0.48787589688314237, 'threshold': 0.7622802703643394}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:04:59,045]\u001b[0m Trial 107 finished with values: [2.300839841365814, 0.8382479071617126, 0.7020000100135804, 0.9203339457511902] and parameters: {'n_hidden': 1, 'n_units': 18, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.5499841382958144, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:05:43,330]\u001b[0m Trial 108 finished with values: [0.688782000541687, 0.8152402400970459, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 5, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 8.446138571374139e-05, 'dropout': 0.8630165121896143, 'threshold': 0.2292333004984978}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:06:24,698]\u001b[0m Trial 109 finished with values: [24.743642807006836, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 0.16904845484147374, 'learning_rate': 0.0003568748649103493, 'dropout': 0.20896209008607705, 'threshold': 0.27265874826006264}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:07:20,379]\u001b[0m Trial 110 finished with values: [0.649104118347168, 0.9067474007606506, 0.8399999976158142, 0.8341826438903809] and parameters: {'n_hidden': 9, 'n_units': 54, 'kernel_regularizer': 2.0678394943630145e-05, 'learning_rate': 6.444708842918975e-05, 'dropout': 0.48726918436401034, 'threshold': 0.7581930049210841}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:08:03,471]\u001b[0m Trial 111 finished with values: [0.9225146174430847, 0.9068171858787537, 0.8399999976158142, 0.8772445559501648] and parameters: {'n_hidden': 3, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0005834382440750414, 'dropout': 0.8662788901341136, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:08:45,779]\u001b[0m Trial 112 finished with values: [0.7260172963142395, 0.7130699276924133, 0.5900000035762787, 0.3845200382173061] and parameters: {'n_hidden': 3, 'n_units': 42, 'kernel_regularizer': 3.252320601075059e-05, 'learning_rate': 1.1267341008755919e-05, 'dropout': 0.21382376483484197, 'threshold': 0.5561347996980976}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:09:33,405]\u001b[0m Trial 113 finished with values: [161.9451110839844, 0.5155083775520325, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 4, 'n_units': 64, 'kernel_regularizer': 0.04752892339894869, 'learning_rate': 1.4988822340920365e-05, 'dropout': 0.3188170831736297, 'threshold': 0.8619292156439865}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:10:26,892]\u001b[0m Trial 114 finished with values: [1.1643304467201232, 0.8885022878646851, 0.8399999856948852, 0.8301518559455872] and parameters: {'n_hidden': 10, 'n_units': 44, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.0005834382440750414, 'dropout': 0.04729505088723851, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:11:14,904]\u001b[0m Trial 115 finished with values: [10.696697235107422, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 46, 'kernel_regularizer': 0.05778245675226843, 'learning_rate': 0.021855574338412464, 'dropout': 0.3395203701483684, 'threshold': 0.38558963176039396}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:11:53,594]\u001b[0m Trial 116 finished with values: [0.6622476935386657, 0.8435366034507752, 0.7699999809265137, 0.6557287335395813] and parameters: {'n_hidden': 1, 'n_units': 18, 'kernel_regularizer': 0.00016585394479730057, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.8077040619685475, 'threshold': 0.6021964792751174}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:12:33,271]\u001b[0m Trial 117 finished with values: [1.109218215942383, 0.9023227572441102, 0.8200000047683715, 0.9125637292861939] and parameters: {'n_hidden': 1, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.12198972576711176, 'dropout': 0.6376447344086704, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:13:25,114]\u001b[0m Trial 118 finished with values: [10.286437797546387, 0.6986342310905457, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 0.004003954888836428, 'learning_rate': 8.446138571374139e-05, 'dropout': 0.8418975623097472, 'threshold': 0.52263525360727}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:14:21,739]\u001b[0m Trial 119 finished with values: [3.287533140182495, 0.8203594446182251, 0.8059999942779541, 0.7922435283660889] and parameters: {'n_hidden': 10, 'n_units': 52, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.012576086124314958, 'dropout': 0.8077040619685475, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:15:01,131]\u001b[0m Trial 120 finished with values: [0.40059908032417296, 0.9198585271835327, 0.8319999933242798, 0.8680162310600281] and parameters: {'n_hidden': 1, 'n_units': 33, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.1697866286480649, 'threshold': 0.48870052508813955}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:15:38,827]\u001b[0m Trial 121 finished with values: [3.232883167266846, 0.8304780840873718, 0.7639999985694885, 0.6381268486380577] and parameters: {'n_hidden': 1, 'n_units': 24, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.5499841382958144, 'threshold': 0.5999518187639323}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:16:21,028]\u001b[0m Trial 122 finished with values: [1.3702295541763305, 0.8980507135391236, 0.822000014781952, 0.8580650091171265] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 0.018455732471100276, 'learning_rate': 0.007543213686442456, 'dropout': 0.7646124647190425, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:17:10,110]\u001b[0m Trial 123 finished with values: [6.128531813621521, 0.8459521412849427, 0.822000014781952, 0.8467972278594971] and parameters: {'n_hidden': 7, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.037826567206786905, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:17:49,190]\u001b[0m Trial 124 finished with values: [0.8516026139259338, 0.4834596157073975, 0.4880000054836273, 0.9367346882820129] and parameters: {'n_hidden': 2, 'n_units': 8, 'kernel_regularizer': 0.0004416315616331735, 'learning_rate': 1.0619818967848172e-05, 'dropout': 0.35889713741090246, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:18:36,203]\u001b[0m Trial 125 finished with values: [1.0822391629219055, 0.8860573649406434, 0.8240000009536743, 0.8566989660263061] and parameters: {'n_hidden': 6, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0006521686023472134, 'dropout': 0.2862399358316623, 'threshold': 0.28484961135147746}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:19:22,467]\u001b[0m Trial 126 finished with values: [2.838380765914917, 0.8335960030555725, 0.8100000023841858, 0.8316113591194153] and parameters: {'n_hidden': 6, 'n_units': 16, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.37889332143468546, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:20:11,706]\u001b[0m Trial 127 finished with values: [1.5607510089874268, 0.8573141098022461, 0.8320000052452088, 0.8758908748626709] and parameters: {'n_hidden': 9, 'n_units': 16, 'kernel_regularizer': 0.00023198013313174801, 'learning_rate': 0.009763246058742354, 'dropout': 0.11174948366441749, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:21:00,062]\u001b[0m Trial 128 finished with values: [6.7017388343811035, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 7, 'n_units': 34, 'kernel_regularizer': 0.018455732471100276, 'learning_rate': 0.06347803812641022, 'dropout': 0.7646124647190425, 'threshold': 0.1394364965085842}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:21:42,948]\u001b[0m Trial 129 finished with values: [1.7001613795757293, 0.8728554010391235, 0.8420000076293945, 0.8582560300827027] and parameters: {'n_hidden': 5, 'n_units': 16, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.7860688156703391, 'threshold': 0.7347123009931876}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:23:02,123]\u001b[0m Trial 130 finished with values: [0.9502169013023376, 0.9027291774749756, 0.8120000004768372, 0.8734803795814514] and parameters: {'n_hidden': 1, 'n_units': 64, 'kernel_regularizer': 1.8006114411320668e-05, 'learning_rate': 0.021296428950781383, 'dropout': 0.8662788901341136, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:24:34,226]\u001b[0m Trial 131 finished with values: [4.767767667770386, 0.8326123237609864, 0.8239999890327454, 0.9023624062538147] and parameters: {'n_hidden': 6, 'n_units': 37, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.6571075663804247, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:25:40,923]\u001b[0m Trial 132 finished with values: [1.011637020111084, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 7, 'n_units': 19, 'kernel_regularizer': 0.034216289879073276, 'learning_rate': 0.0035197135737012686, 'dropout': 0.363098274505335, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:26:47,246]\u001b[0m Trial 133 finished with values: [0.5716679573059082, 0.905258584022522, 0.7259999871253967, 0.9639510750770569] and parameters: {'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 0.019189432566192953, 'learning_rate': 0.0007970799285309584, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:27:38,473]\u001b[0m Trial 134 finished with values: [2.093587565422058, 0.8563916325569153, 0.8300000071525574, 0.866187059879303] and parameters: {'n_hidden': 6, 'n_units': 16, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.8222956921294372, 'threshold': 0.4826296718344151}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:28:20,279]\u001b[0m Trial 135 finished with values: [0.6448575377464294, 0.9083247900009155, 0.6620000004768372, 0.9678155660629273] and parameters: {'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 0.00023198013313174801, 'learning_rate': 7.015653993460166e-05, 'dropout': 0.11174948366441749, 'threshold': 0.31665717730358856}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:29:11,034]\u001b[0m Trial 136 finished with values: [22.085847854614258, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 9, 'n_units': 32, 'kernel_regularizer': 0.6907933918051181, 'learning_rate': 0.005952816600700391, 'dropout': 0.1541069665813928, 'threshold': 0.52263525360727}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:29:54,620]\u001b[0m Trial 137 finished with values: [25.682260513305664, 0.5535448908805847, 0.5599999904632569, 0.1] and parameters: {'n_hidden': 3, 'n_units': 44, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.5743365897491732, 'dropout': 0.20902702870023096, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:30:36,008]\u001b[0m Trial 138 finished with values: [3.4057414054870607, 0.7233860850334167, 0.6759999990463257, 0.3891582489013672] and parameters: {'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 0.0010579205679920412, 'learning_rate': 0.2794583648150599, 'dropout': 0.2862399358316623, 'threshold': 0.8895057565427082}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:31:21,973]\u001b[0m Trial 139 finished with values: [151.49151306152345, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 3, 'n_units': 60, 'kernel_regularizer': 0.04752892339894869, 'learning_rate': 0.4356979856277655, 'dropout': 0.12113634180782347, 'threshold': 0.37404947489402296}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:32:01,582]\u001b[0m Trial 140 finished with values: [0.5799643695354462, 0.7292367100715638, 0.6959999918937683, 0.8786394596099854] and parameters: {'n_hidden': 3, 'n_units': 2, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.0012602416730026099, 'dropout': 0.6509758120809551, 'threshold': 0.3893046333814376}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:32:48,528]\u001b[0m Trial 141 finished with values: [0.7802004218101501, 0.572323226928711, 0.5780000030994416, 0.16727272272109986] and parameters: {'n_hidden': 7, 'n_units': 11, 'kernel_regularizer': 0.010667351052332364, 'learning_rate': 0.005679615302586204, 'dropout': 0.8077040619685475, 'threshold': 0.6021964792751174}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:33:26,392]\u001b[0m Trial 142 finished with values: [0.6702943265438079, 0.9016819000244141, 0.8100000023841858, 0.8722091794013977] and parameters: {'n_hidden': 1, 'n_units': 22, 'kernel_regularizer': 0.000918416956060503, 'learning_rate': 0.00378997837281349, 'dropout': 0.2862399358316623, 'threshold': 0.25478675673102114}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:34:07,308]\u001b[0m Trial 143 finished with values: [0.5137986361980438, 0.917686939239502, 0.8199999928474426, 0.9147296190261841] and parameters: {'n_hidden': 1, 'n_units': 37, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0006521686023472134, 'dropout': 0.048525896101412724, 'threshold': 0.27265874826006264}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:34:59,515]\u001b[0m Trial 144 finished with values: [2.753468465805054, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 3.252320601075059e-05, 'learning_rate': 0.4356979856277655, 'dropout': 0.8630165121896143, 'threshold': 0.37404947489402296}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:35:48,570]\u001b[0m Trial 145 finished with values: [13.439428520202636, 0.785930585861206, 0.7520000100135803, 0.866248881816864] and parameters: {'n_hidden': 7, 'n_units': 33, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.06347803812641022, 'dropout': 0.16323807071529978, 'threshold': 0.1394364965085842}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:36:34,925]\u001b[0m Trial 146 finished with values: [1.0667168736457824, 0.510869562625885, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 5, 'n_units': 42, 'kernel_regularizer': 0.00016585394479730057, 'learning_rate': 0.13216459374881756, 'dropout': 0.8077040619685475, 'threshold': 0.7610837794265474}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:37:20,894]\u001b[0m Trial 147 finished with values: [0.81191166639328, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 6, 'kernel_regularizer': 0.05778245675226843, 'learning_rate': 0.0027041786234411743, 'dropout': 0.48726918436401034, 'threshold': 0.27938857146053786}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:38:10,236]\u001b[0m Trial 148 finished with values: [2.619136166572571, 0.8303048849105835, 0.8179999947547912, 0.8568570017814636] and parameters: {'n_hidden': 6, 'n_units': 18, 'kernel_regularizer': 0.00016585394479730057, 'learning_rate': 0.012576086124314958, 'dropout': 0.7010918993652716, 'threshold': 0.6021964792751174}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:38:49,324]\u001b[0m Trial 149 finished with values: [1.1185072898864745, 0.892025101184845, 0.8099999904632569, 0.8598969340324402] and parameters: {'n_hidden': 1, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.021296428950781383, 'dropout': 0.6376447344086704, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:39:27,134]\u001b[0m Trial 150 finished with values: [0.4687472879886627, 0.9205464839935302, 0.8259999871253967, 0.8815089583396911] and parameters: {'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.3923850775943305}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:40:08,603]\u001b[0m Trial 151 finished with values: [1.3659975051879882, 0.891447103023529, 0.8120000004768372, 0.8714271903038024] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 0.018455732471100276, 'learning_rate': 0.007543213686442456, 'dropout': 0.7646124647190425, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:40:55,464]\u001b[0m Trial 152 finished with values: [1.078835391998291, 0.8190890312194824, 0.7480000019073486, 0.6649130821228028] and parameters: {'n_hidden': 3, 'n_units': 55, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 1.4988822340920365e-05, 'dropout': 0.3395203701483684, 'threshold': 0.5076596800402231}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:41:34,564]\u001b[0m Trial 153 finished with values: [0.792574667930603, 0.9216447710990906, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 2, 'n_units': 11, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 0.00039626071240343367, 'dropout': 0.8568014240464407, 'threshold': 0.10873515186137227}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:42:17,357]\u001b[0m Trial 154 finished with values: [1.43830304145813, 0.7853299617767334, 0.7040000081062316, 0.4484271354973316] and parameters: {'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.44869763049605843, 'threshold': 0.7209221107799245}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:43:00,039]\u001b[0m Trial 155 finished with values: [0.8515121579170227, 0.8955576658248902, 0.8419999957084656, 0.8793609499931335] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:43:46,416]\u001b[0m Trial 156 finished with values: [1.1106877446174621, 0.8910589814186096, 0.8020000100135803, 0.8857252836227417] and parameters: {'n_hidden': 3, 'n_units': 53, 'kernel_regularizer': 0.014191818130035913, 'learning_rate': 0.00378997837281349, 'dropout': 0.3395203701483684, 'threshold': 0.25478675673102114}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:44:28,227]\u001b[0m Trial 157 finished with values: [3.1777288913726807, 0.7413449048995971, 0.49200000166893004, 1.0] and parameters: {'n_hidden': 3, 'n_units': 27, 'kernel_regularizer': 0.0021485442537407993, 'learning_rate': 1.4988822340920365e-05, 'dropout': 0.048525896101412724, 'threshold': 0.31665717730358856}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:45:25,663]\u001b[0m Trial 158 finished with values: [11.534132766723634, 0.8388338685035706, 0.8080000042915344, 0.8786284804344178] and parameters: {'n_hidden': 9, 'n_units': 64, 'kernel_regularizer': 4.099647136626668e-05, 'learning_rate': 0.021296428950781383, 'dropout': 0.8630165121896143, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:46:15,291]\u001b[0m Trial 159 finished with values: [2.72382390499115, 0.8153521060943604, 0.8039999961853027, 0.8400316119194031] and parameters: {'n_hidden': 9, 'n_units': 10, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.009763246058742354, 'dropout': 0.11174948366441749, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:47:04,021]\u001b[0m Trial 160 finished with values: [1.1665396809577941, 0.8743515729904174, 0.8199999928474426, 0.8668535709381103] and parameters: {'n_hidden': 9, 'n_units': 6, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0013414232848841775, 'dropout': 0.48726918436401034, 'threshold': 0.331161422068207}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:47:44,387]\u001b[0m Trial 161 finished with values: [1.9924844026565551, 0.8778916835784912, 0.8220000028610229, 0.8798392057418823] and parameters: {'n_hidden': 1, 'n_units': 33, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.5743365897491732, 'dropout': 0.4696185332585968, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:48:23,718]\u001b[0m Trial 162 finished with values: [0.5697926878929138, 0.8191801548004151, 0.6319999992847443, 0.950841748714447] and parameters: {'n_hidden': 3, 'n_units': 2, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.0012602416730026099, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:49:08,443]\u001b[0m Trial 163 finished with values: [2.7242265939712524, 0.8348466873168945, 0.8120000004768372, 0.8656112074851989] and parameters: {'n_hidden': 5, 'n_units': 16, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:49:46,097]\u001b[0m Trial 164 finished with values: [0.787497055530548, 0.5232026100158691, 0.5060000002384186, 1.0] and parameters: {'n_hidden': 1, 'n_units': 6, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.6061345542965496, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:50:34,937]\u001b[0m Trial 165 finished with values: [1.9890889406204224, 0.8613701343536377, 0.8220000028610229, 0.9013261795043945] and parameters: {'n_hidden': 9, 'n_units': 16, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.3395203701483684, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:51:28,517]\u001b[0m Trial 166 finished with values: [1.502289628982544, 0.8929004311561585, 0.8279999971389771, 0.8551034212112427] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 0.0004081577143659623, 'learning_rate': 0.00020655775452336643, 'dropout': 0.10114145915245089, 'threshold': 0.5999518187639323}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:52:12,488]\u001b[0m Trial 167 finished with values: [46.51184158325195, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 0.018455732471100276, 'learning_rate': 0.5743365897491732, 'dropout': 0.20902702870023096, 'threshold': 0.3825476197746178}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:52:55,233]\u001b[0m Trial 168 finished with values: [0.9447278141975403, 0.8774121761322021, 0.8039999961853027, 0.8734405279159546] and parameters: {'n_hidden': 3, 'n_units': 22, 'kernel_regularizer': 0.000918416956060503, 'learning_rate': 0.00378997837281349, 'dropout': 0.048525896101412724, 'threshold': 0.27265874826006264}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:53:42,369]\u001b[0m Trial 169 finished with values: [4.212230396270752, 0.7267897367477417, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 6, 'kernel_regularizer': 0.05778245675226843, 'learning_rate': 0.00020655775452336643, 'dropout': 0.3395203701483684, 'threshold': 0.25640222221858333}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:54:28,083]\u001b[0m Trial 170 finished with values: [0.691960608959198, 0.6503933846950531, 0.5459999918937684, 0.6836116343736649] and parameters: {'n_hidden': 6, 'n_units': 12, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 1.1875114388657349e-05, 'dropout': 0.1697866286480649, 'threshold': 0.48870052508813955}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:55:16,714]\u001b[0m Trial 171 finished with values: [0.6078463077545166, 0.8856501817703247, 0.6, 0.179818594455719] and parameters: {'n_hidden': 9, 'n_units': 4, 'kernel_regularizer': 7.059034212989332e-05, 'learning_rate': 0.00020655775452336643, 'dropout': 0.35874953861229797, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:55:56,455]\u001b[0m Trial 172 finished with values: [0.7523334264755249, 0.8984099149703979, 0.8279999971389771, 0.8772445559501648] and parameters: {'n_hidden': 1, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.0035197135737012686, 'dropout': 0.6376447344086704, 'threshold': 0.2808948817742202}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:56:44,337]\u001b[0m Trial 173 finished with values: [1.0963311076164246, 0.8722777843475342, 0.8139999985694886, 0.8636006355285645] and parameters: {'n_hidden': 6, 'n_units': 37, 'kernel_regularizer': 0.0010579205679920412, 'learning_rate': 0.0049473859340445365, 'dropout': 0.2862399358316623, 'threshold': 0.1394364965085842}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:57:30,812]\u001b[0m Trial 174 finished with values: [1.942827296257019, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 5, 'n_units': 33, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.5743365897491732, 'dropout': 0.16323807071529978, 'threshold': 0.1394364965085842}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:58:14,730]\u001b[0m Trial 175 finished with values: [0.3934956967830658, 0.9221025228500366, 0.8300000071525574, 0.844469177722931] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.48870052508813955}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:58:54,604]\u001b[0m Trial 176 finished with values: [0.7271264910697937, 0.8994136571884155, 0.8279999971389771, 0.7765725255012512] and parameters: {'n_hidden': 1, 'n_units': 37, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0035197135737012686, 'dropout': 0.45781031663034927, 'threshold': 0.817729377535572}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 00:59:38,931]\u001b[0m Trial 177 finished with values: [0.7082760453224182, 0.5609359443187714, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 6, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 1.1875114388657349e-05, 'dropout': 0.8222956921294372, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:00:18,191]\u001b[0m Trial 178 finished with values: [2.9840699672698974, 0.8047267079353333, 0.6079999923706054, 0.22107743918895723] and parameters: {'n_hidden': 2, 'n_units': 11, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.10114145915245089, 'threshold': 0.5999518187639323}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:01:08,655]\u001b[0m Trial 179 finished with values: [5.281620931625366, 0.5, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 9, 'n_units': 16, 'kernel_regularizer': 0.00032005478454991034, 'learning_rate': 0.5950638566620096, 'dropout': 0.8447879213862368, 'threshold': 0.52263525360727}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:01:51,106]\u001b[0m Trial 180 finished with values: [0.9155596137046814, 0.9115429043769836, 0.8360000133514405, 0.9083584070205688] and parameters: {'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 0.0005069808946385528, 'learning_rate': 0.021296428950781383, 'dropout': 0.8662788901341136, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:02:34,083]\u001b[0m Trial 181 finished with values: [1.4890463948249817, 0.8475726962089538, 0.8080000042915344, 0.817421841621399] and parameters: {'n_hidden': 3, 'n_units': 16, 'kernel_regularizer': 0.000918416956060503, 'learning_rate': 0.009613908080901729, 'dropout': 0.4373168502240487, 'threshold': 0.5561129202357704}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:03:23,141]\u001b[0m Trial 182 finished with values: [0.6690444469451904, 0.9135642051696777, 0.5700000047683715, 0.9904761910438538] and parameters: {'n_hidden': 9, 'n_units': 8, 'kernel_regularizer': 0.0004416315616331735, 'learning_rate': 0.00020655775452336643, 'dropout': 0.5039281376641517, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:04:09,552]\u001b[0m Trial 183 finished with values: [0.7641909122467041, 0.5761212110519409, 0.5900000035762787, 0.17454545497894286] and parameters: {'n_hidden': 7, 'n_units': 6, 'kernel_regularizer': 0.010749981310237407, 'learning_rate': 0.00772064391502971, 'dropout': 0.48787589688314237, 'threshold': 0.7622802703643394}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:04:52,936]\u001b[0m Trial 184 finished with values: [0.660084354877472, 0.8397876501083374, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 7.015653993460166e-05, 'dropout': 0.7860688156703391, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:05:32,147]\u001b[0m Trial 185 finished with values: [0.7380780577659607, 0.9013388872146606, 0.8259999990463257, 0.8695416808128357] and parameters: {'n_hidden': 1, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.0035197135737012686, 'dropout': 0.6376447344086704, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:06:24,256]\u001b[0m Trial 186 finished with values: [0.47259876132011414, 0.9118376135826111, 0.8140000104904175, 0.853971004486084] and parameters: {'n_hidden': 9, 'n_units': 33, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.10114145915245089, 'threshold': 0.48870052508813955}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:07:14,478]\u001b[0m Trial 187 finished with values: [1.5650161027908325, 0.8655600666999816, 0.8240000128746032, 0.8716745734214782] and parameters: {'n_hidden': 7, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.3395203701483684, 'threshold': 0.2914451882161558}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:08:04,961]\u001b[0m Trial 188 finished with values: [1.9887268781661986, 0.8236507058143616, 0.8, 0.8578478693962097] and parameters: {'n_hidden': 10, 'n_units': 16, 'kernel_regularizer': 0.00023198013313174801, 'learning_rate': 0.009763246058742354, 'dropout': 0.11174948366441749, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:08:52,105]\u001b[0m Trial 189 finished with values: [0.6855489492416382, 0.8414567112922668, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 7, 'n_units': 14, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 4.9567167614517136e-05, 'dropout': 0.6061345542965496, 'threshold': 0.2292333004984978}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:09:45,987]\u001b[0m Trial 190 finished with values: [2.422376251220703, 0.8432271957397461, 0.8399999976158142, 0.8891788721084595] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 1.8148712421091602e-05, 'learning_rate': 0.002389600411589469, 'dropout': 0.48787589688314237, 'threshold': 0.3814482156016251}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:10:25,512]\u001b[0m Trial 191 finished with values: [0.4412053942680359, 0.9197731733322143, 0.6860000014305114, 0.9664014339447021] and parameters: {'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 9.988480575396382e-05, 'dropout': 0.5499841382958144, 'threshold': 0.27265874826006264}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:11:11,258]\u001b[0m Trial 192 finished with values: [0.9555911540985107, 0.8818184018135071, 0.8159999966621398, 0.8883034586906433] and parameters: {'n_hidden': 6, 'n_units': 11, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 0.0049473859340445365, 'dropout': 0.2862399358316623, 'threshold': 0.10873515186137227}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:11:50,728]\u001b[0m Trial 193 finished with values: [0.5489205896854401, 0.9042056679725647, 0.7239999890327453, 0.9720318794250489] and parameters: {'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 0.019189432566192953, 'learning_rate': 0.0007970799285309584, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:12:30,426]\u001b[0m Trial 194 finished with values: [6.009439849853516, 0.9001696586608887, 0.7960000038146973, 0.882877767086029] and parameters: {'n_hidden': 1, 'n_units': 19, 'kernel_regularizer': 0.019189432566192953, 'learning_rate': 0.12198972576711176, 'dropout': 0.054722990825516016, 'threshold': 0.3318972015983154}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:13:13,643]\u001b[0m Trial 195 finished with values: [0.7484604597091675, 0.8973929166793824, 0.8480000019073486, 0.8014279007911682] and parameters: {'n_hidden': 2, 'n_units': 44, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:13:56,645]\u001b[0m Trial 196 finished with values: [0.6908408641815186, 0.9271596908569336, 0.8119999766349792, 0.9139888763427735] and parameters: {'n_hidden': 1, 'n_units': 64, 'kernel_regularizer': 0.0007793010643204281, 'learning_rate': 0.0003795051194324455, 'dropout': 0.7010918993652716, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:14:38,262]\u001b[0m Trial 197 finished with values: [0.6969481885433197, 0.913212525844574, 0.8120000004768372, 0.8974204659461975] and parameters: {'n_hidden': 2, 'n_units': 44, 'kernel_regularizer': 7.059034212989332e-05, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:15:28,952]\u001b[0m Trial 198 finished with values: [2.689153957366943, 0.8384135007858277, 0.8279999971389771, 0.852617335319519] and parameters: {'n_hidden': 9, 'n_units': 23, 'kernel_regularizer': 7.059034212989332e-05, 'learning_rate': 0.0023136918201300295, 'dropout': 0.5039281376641517, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:16:10,821]\u001b[0m Trial 199 finished with values: [0.5420597970485688, 0.9165560603141785, 0.7740000009536743, 0.938433313369751] and parameters: {'n_hidden': 1, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:16:58,911]\u001b[0m Trial 200 finished with values: [0.5054279148578644, 0.9110304832458496, 0.7900000095367432, 0.9068164587020874] and parameters: {'n_hidden': 7, 'n_units': 25, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 9.988480575396382e-05, 'dropout': 0.5499841382958144, 'threshold': 0.27265874826006264}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:17:44,040]\u001b[0m Trial 201 finished with values: [0.8252065181732178, 0.8987080097198487, 0.8299999952316284, 0.864334499835968] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.0957909047863415e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.8662788901341136, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:18:22,161]\u001b[0m Trial 202 finished with values: [0.5029459714889526, 0.9247026205062866, 0.8399999856948852, 0.8978726029396057] and parameters: {'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.3923850775943305}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:19:06,644]\u001b[0m Trial 203 finished with values: [1.4869099140167237, 0.9124262690544128, 0.5899999856948852, 0.9885109663009644] and parameters: {'n_hidden': 3, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 5.3577811386515144e-05, 'dropout': 0.6571075663804247, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:19:53,263]\u001b[0m Trial 204 finished with values: [0.5312606632709503, 0.9205090522766113, 0.793999993801117, 0.929806911945343] and parameters: {'n_hidden': 7, 'n_units': 14, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0006955292277375732, 'dropout': 0.8959560597731027, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:20:39,163]\u001b[0m Trial 205 finished with values: [0.6842221736907959, 0.8418906331062317, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 5, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 8.446138571374139e-05, 'dropout': 0.8630165121896143, 'threshold': 0.2292333004984978}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:21:27,149]\u001b[0m Trial 206 finished with values: [3.889989471435547, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 33, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.5743365897491732, 'dropout': 0.4696185332585968, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:22:16,417]\u001b[0m Trial 207 finished with values: [0.914600419998169, 0.8913565754890442, 0.8460000038146973, 0.8768144130706788] and parameters: {'n_hidden': 3, 'n_units': 56, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.0012602416730026099, 'dropout': 0.0686413637905968, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:23:05,896]\u001b[0m Trial 208 finished with values: [0.7306662201881409, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 7, 'n_units': 6, 'kernel_regularizer': 3.252320601075059e-05, 'learning_rate': 0.4356979856277655, 'dropout': 0.21382376483484197, 'threshold': 0.27938857146053786}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:23:46,067]\u001b[0m Trial 209 finished with values: [0.6253150463104248, 0.9252760767936706, 0.8200000047683715, 0.6770246863365174] and parameters: {'n_hidden': 1, 'n_units': 37, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.00023630528778335224, 'dropout': 0.2862399358316623, 'threshold': 0.8171312422835556}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:24:38,282]\u001b[0m Trial 210 finished with values: [0.8527446389198303, 0.8961566925048828, 0.7820000052452087, 0.6420188397169113] and parameters: {'n_hidden': 10, 'n_units': 19, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.00020655775452336643, 'dropout': 0.3395203701483684, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:25:29,800]\u001b[0m Trial 211 finished with values: [0.4905992388725281, 0.9058263421058654, 0.8139999866485595, 0.8609881162643432] and parameters: {'n_hidden': 9, 'n_units': 21, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.10114145915245089, 'threshold': 0.48870052508813955}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:26:10,747]\u001b[0m Trial 212 finished with values: [1.3394490957260132, 0.5779517889022827, 0.514000004529953, 0.9056607007980346] and parameters: {'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 0.0005069808946385528, 'learning_rate': 1.2262445892357338e-05, 'dropout': 0.8662788901341136, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:27:00,045]\u001b[0m Trial 213 finished with values: [1.4952080965042114, 0.8681126117706299, 0.8279999852180481, 0.8225328207015992] and parameters: {'n_hidden': 7, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0005834382440750414, 'dropout': 0.6061345542965496, 'threshold': 0.6082921110328658}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:27:49,379]\u001b[0m Trial 214 finished with values: [1.752713942527771, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 6, 'n_units': 25, 'kernel_regularizer': 0.019189432566192953, 'learning_rate': 0.0007970799285309584, 'dropout': 0.6376447344086704, 'threshold': 0.2292333004984978}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:28:36,859]\u001b[0m Trial 215 finished with values: [15.667789459228516, 0.5921490848064422, 0.5099999964237213, 0.0] and parameters: {'n_hidden': 7, 'n_units': 14, 'kernel_regularizer': 0.024636523154149638, 'learning_rate': 4.9567167614517136e-05, 'dropout': 0.6061345542965496, 'threshold': 0.7101191097233752}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:29:22,938]\u001b[0m Trial 216 finished with values: [2.1921540200710297, 0.8558064699172974, 0.8159999966621398, 0.8667436242103577] and parameters: {'n_hidden': 4, 'n_units': 41, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.6571075663804247, 'threshold': 0.48870052508813955}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:30:07,529]\u001b[0m Trial 217 finished with values: [0.8858062028884888, 0.8929883241653442, 0.828000009059906, 0.8695416808128357] and parameters: {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:31:06,253]\u001b[0m Trial 218 finished with values: [2.73683557510376, 0.8912436723709106, 0.7420000016689301, 0.9026111483573913] and parameters: {'n_hidden': 9, 'n_units': 64, 'kernel_regularizer': 0.0021485442537407993, 'learning_rate': 0.0003795051194324455, 'dropout': 0.29751922113504004, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:31:46,504]\u001b[0m Trial 219 finished with values: [68.16271209716797, 0.5518209338188171, 0.5299999952316284, 0.6887500911951066] and parameters: {'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 0.09629889876172265, 'learning_rate': 3.676021630971176e-05, 'dropout': 0.5499841382958144, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:32:37,649]\u001b[0m Trial 220 finished with values: [1.6588363647460938, 0.8564007163047791, 0.8360000014305115, 0.8806307911872864] and parameters: {'n_hidden': 7, 'n_units': 37, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.0013414232848841775, 'dropout': 0.6061345542965496, 'threshold': 0.27938857146053786}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:33:19,001]\u001b[0m Trial 221 finished with values: [0.9385255098342895, 0.9120967507362365, 0.8199999928474426, 0.8644252061843872] and parameters: {'n_hidden': 1, 'n_units': 33, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.6235818455425759, 'threshold': 0.48870052508813955}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:33:57,575]\u001b[0m Trial 222 finished with values: [0.5315422773361206, 0.9294293642044067, 0.8379999995231628, 0.7234522104263306] and parameters: {'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 0.0017899630867169573, 'learning_rate': 0.0005834382440750414, 'dropout': 0.7860688156703391, 'threshold': 0.7347123009931876}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:34:40,733]\u001b[0m Trial 223 finished with values: [0.6485214829444885, 0.9112820863723755, 0.8320000052452088, 0.7712911486625671] and parameters: {'n_hidden': 2, 'n_units': 44, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.0007970799285309584, 'dropout': 0.04729505088723851, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:35:19,620]\u001b[0m Trial 224 finished with values: [0.9425580143928528, 0.5794578790664673, 0.49999999403953554, 1.0] and parameters: {'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.6061345542965496, 'threshold': 0.1325102969894017}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:36:17,747]\u001b[0m Trial 225 finished with values: [3.784447717666626, 0.8267827033996582, 0.8259999990463257, 0.8673359513282776] and parameters: {'n_hidden': 9, 'n_units': 54, 'kernel_regularizer': 2.0678394943630145e-05, 'learning_rate': 0.005882814791969448, 'dropout': 0.48726918436401034, 'threshold': 0.7009432385718232}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:36:58,295]\u001b[0m Trial 226 finished with values: [0.5542078018188477, 0.9208210825920105, 0.8340000033378601, 0.7458256125450134] and parameters: {'n_hidden': 1, 'n_units': 36, 'kernel_regularizer': 0.0007793010643204281, 'learning_rate': 0.0007970799285309584, 'dropout': 0.7010918993652716, 'threshold': 0.8252501260451032}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:37:39,474]\u001b[0m Trial 227 finished with values: [0.5623070478439331, 0.9282918453216553, 0.8379999995231628, 0.7135408520698547] and parameters: {'n_hidden': 1, 'n_units': 33, 'kernel_regularizer': 0.0007793010643204281, 'learning_rate': 0.0003795051194324455, 'dropout': 0.4696185332585968, 'threshold': 0.8252501260451032}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:38:30,599]\u001b[0m Trial 228 finished with values: [1.2998634934425355, 0.8729795694351197, 0.85, 0.864334499835968] and parameters: {'n_hidden': 5, 'n_units': 33, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.001598284971321218, 'dropout': 0.20902702870023096, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:39:10,974]\u001b[0m Trial 229 finished with values: [0.5484106063842773, 0.8117827534675598, 0.6900000095367431, 0.9066213130950928] and parameters: {'n_hidden': 3, 'n_units': 2, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.0008337729375459277, 'dropout': 0.6061345542965496, 'threshold': 0.3893046333814376}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:39:51,122]\u001b[0m Trial 230 finished with values: [0.6875609874725341, 0.6539403200149536, 0.5399999976158142, 0.1427815556526184] and parameters: {'n_hidden': 3, 'n_units': 2, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.29992301899310914, 'threshold': 0.5952829051960941}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:40:44,425]\u001b[0m Trial 231 finished with values: [1.7760914325714112, 0.850358498096466, 0.8159999966621398, 0.859490156173706] and parameters: {'n_hidden': 9, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:41:24,150]\u001b[0m Trial 232 finished with values: [0.4503865480422974, 0.917121171951294, 0.8240000009536743, 0.8650601267814636] and parameters: {'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.3923850775943305}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:42:22,168]\u001b[0m Trial 233 finished with values: [4.884823942184449, 0.8353386640548706, 0.8279999852180481, 0.881326186656952] and parameters: {'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 1.8148712421091602e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.48787589688314237, 'threshold': 0.3814482156016251}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:43:15,516]\u001b[0m Trial 234 finished with values: [2.766863250732422, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 10, 'n_units': 19, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.6376447344086704, 'threshold': 0.2292333004984978}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:43:56,568]\u001b[0m Trial 235 finished with values: [2.891525483131409, 0.8313444375991821, 0.8120000123977661, 0.8525279998779297] and parameters: {'n_hidden': 3, 'n_units': 8, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.022780523161934768, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:44:52,974]\u001b[0m Trial 236 finished with values: [1.2402051210403442, 0.8994691729545593, 0.8460000038146973, 0.8315398812294006] and parameters: {'n_hidden': 10, 'n_units': 44, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.00012248524389842045, 'dropout': 0.3395203701483684, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:45:36,179]\u001b[0m Trial 237 finished with values: [10.490232467651367, 0.8669434905052185, 0.49799999594688416, 1.0] and parameters: {'n_hidden': 2, 'n_units': 42, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 4.9567167614517136e-05, 'dropout': 0.08891052102456992, 'threshold': 0.2292333004984978}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:46:24,986]\u001b[0m Trial 238 finished with values: [4.0463111877441404, 0.8315989971160889, 0.8019999861717224, 0.8323287248611451] and parameters: {'n_hidden': 6, 'n_units': 34, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.021296428950781383, 'dropout': 0.8662788901341136, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:47:09,474]\u001b[0m Trial 239 finished with values: [0.6841946721076966, 0.9081566572189331, 0.8480000019073486, 0.8039387106895447] and parameters: {'n_hidden': 2, 'n_units': 44, 'kernel_regularizer': 1.1874693702272719e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.8929033947032199}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:47:48,597]\u001b[0m Trial 240 finished with values: [1.2156963348388672, 0.9073869347572326, 0.8139999985694886, 0.8820957899093628] and parameters: {'n_hidden': 1, 'n_units': 21, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.12198972576711176, 'dropout': 0.6376447344086704, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:48:30,405]\u001b[0m Trial 241 finished with values: [2.1088787078857423, 0.9027494430541992, 0.49200000166893004, 1.0] and parameters: {'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 0.12254567223517063, 'learning_rate': 0.003763855754353628, 'dropout': 0.8662788901341136, 'threshold': 0.3544065415833555}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:49:23,436]\u001b[0m Trial 242 finished with values: [2.6108381748199463, 0.8177525997161865, 0.8019999980926513, 0.8503868699073791] and parameters: {'n_hidden': 9, 'n_units': 27, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 0.009613908080901729, 'dropout': 0.23290423356661158, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:50:12,791]\u001b[0m Trial 243 finished with values: [1.274531853199005, 0.8690256357192994, 0.8239999890327454, 0.827208149433136] and parameters: {'n_hidden': 5, 'n_units': 44, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.002389600411589469, 'dropout': 0.20902702870023096, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:51:11,852]\u001b[0m Trial 244 finished with values: [3.3259518146514893, 0.8994499325752259, 0.7460000038146972, 0.5821164131164551] and parameters: {'n_hidden': 9, 'n_units': 54, 'kernel_regularizer': 0.0007793010643204281, 'learning_rate': 6.444708842918975e-05, 'dropout': 0.48726918436401034, 'threshold': 0.7581930049210841}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:51:55,371]\u001b[0m Trial 245 finished with values: [0.4241879940032959, 0.9227930545806885, 0.8200000047683715, 0.8820064544677735] and parameters: {'n_hidden': 1, 'n_units': 58, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0003795051194324455, 'dropout': 0.20896209008607705, 'threshold': 0.383494481615293}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:52:52,193]\u001b[0m Trial 246 finished with values: [4.175232839584351, 0.5, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 10, 'n_units': 33, 'kernel_regularizer': 5.717837426733353e-05, 'learning_rate': 0.5743365897491732, 'dropout': 0.6376447344086704, 'threshold': 0.4195383350653711}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:53:40,143]\u001b[0m Trial 247 finished with values: [24.47243800163269, 0.6742610216140748, 0.6299999952316284, 0.41809248328208926] and parameters: {'n_hidden': 3, 'n_units': 42, 'kernel_regularizer': 3.252320601075059e-05, 'learning_rate': 0.4356979856277655, 'dropout': 0.29992301899310914, 'threshold': 0.5382301839589044}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:54:29,458]\u001b[0m Trial 248 finished with values: [4.865674304962158, 0.8247377276420593, 0.8040000200271606, 0.8758372902870178] and parameters: {'n_hidden': 3, 'n_units': 64, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.021296428950781383, 'dropout': 0.8662788901341136, 'threshold': 0.3056244326515286}. \u001b[0m\n",
      "\u001b[32m[I 2023-09-11 01:55:16,736]\u001b[0m Trial 249 finished with values: [0.6604617476463318, 0.888714611530304, 0.4900000035762787, 1.0] and parameters: {'n_hidden': 7, 'n_units': 14, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 4.9567167614517136e-05, 'dropout': 0.545499225702559, 'threshold': 0.2292333004984978}. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def create_ann_model(trial):\n",
    "    \n",
    "    n_hidden = trial.suggest_int('n_hidden', 1, 10)\n",
    "    n_units = trial.suggest_int('n_units', 1, 64)\n",
    "    \n",
    "    #hidden_activation = trial.suggest_categorical('activation', ['elu', 'relu'])\n",
    "    k_regularizer = trial.suggest_float('kernel_regularizer', 1e-5, 1e-0, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-0, log=True)\n",
    "    \n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.9)\n",
    "    threshold = trial.suggest_float('threshold', 0.1, 0.9)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape = (1000,)))\n",
    "\n",
    "    for i in range(n_hidden):\n",
    "        \n",
    "        model.add(keras.layers.Dense(n_units, activation='relu', kernel_regularizer=regularizers.L1(k_regularizer)))\n",
    "\n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "                             \n",
    "    \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False\n",
    "                                                   ),\n",
    "                 optimizer = keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                 metrics=['AUC', keras.metrics.BinaryAccuracy(threshold=threshold),\n",
    "                                 keras.metrics.Recall(thresholds=threshold),\n",
    "                                 keras.metrics.Precision(thresholds=threshold)])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def objective(trial):\n",
    "    \n",
    "    loss_list = []; auc_list = []; acc_list = []; rec_list = []; prec_list = []\n",
    "    \n",
    "    for t, v in five_fold:\n",
    "        \n",
    "        # divide into X and y and convert the text into features\n",
    "        X_train = vec.transform(t['abstract'].values).toarray(); y_train = t['label'].values\n",
    "        X_test = vec.transform(v['abstract'].values).toarray(); y_test = v['label'].values\n",
    "        \n",
    "        model = create_ann_model(trial)\n",
    "    \n",
    "        model.fit(X_train, y_train, validation_split=0.25, verbose=0, epochs=100, batch_size=100)\n",
    "    \n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "        loss_list.append(score[0])\n",
    "        auc_list.append(score[1])\n",
    "        acc_list.append(score[2])\n",
    "        rec_list.append(score[3])\n",
    "        prec_list.append(score[4])\n",
    "    \n",
    "    return np.array(loss_list).mean(), np.array(auc_list).mean(), np.array(acc_list).mean(), np.array(rec_list).mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study(study_name='BoW-features',\n",
    "                            directions=['minimize', 'maximize', 'maximize', 'maximize'],\n",
    "                            sampler=optuna.samplers.NSGAIISampler(), \n",
    "                            pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "study.optimize(objective, n_trials=250, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(study.trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/hyperparameter_search/bag_of_words/BoW_ann.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df = study.trials_dataframe()\n",
    "bow_df = bow_df.rename(columns=({'values_0': 'accuracy', 'values_1': 'recall', 'values_2': 'f1', 'values_3': 'precison'}))\n",
    "bow_df.to_json('data/hyperparameter_search/bag_of_words/BoW_ann.json')\n",
    "\n",
    "joblib.dump(study, 'data/hyperparameter_search/bag_of_words/BoW_ann.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model#:  1\n",
      "parameter_set:  {'n_hidden': 9, 'n_units': 5, 'kernel_regularizer': 0.00035721607432455795, 'learning_rate': 0.00014802897107670955, 'dropout': 0.23753685374568326, 'threshold': 0.501941517418462}\n",
      "model#:  2\n",
      "parameter_set:  {'n_hidden': 4, 'n_units': 39, 'kernel_regularizer': 0.010435986651078987, 'learning_rate': 0.0017945330854873274, 'dropout': 0.420905983458333, 'threshold': 0.9223544331901787}\n",
      "model#:  7\n",
      "parameter_set:  {'n_hidden': 5, 'n_units': 40, 'kernel_regularizer': 0.0009997092744088631, 'learning_rate': 0.0003593830342512407, 'dropout': 0.3001342570565156, 'threshold': 0.8043402703667024}\n"
     ]
    }
   ],
   "source": [
    "for i in study.best_trials:\n",
    "    print('model#: ', i.number)\n",
    "    #print('performance: ', i.values)\n",
    "    print('parameter_set: ', i.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model#:  0\n",
      "performance:  [1.1136011362075806, 0.6867601633071899, 0.5020000040531158, 1.0]\n",
      "parameter_set:  {'n_hidden': 7, 'n_units': 62, 'kernel_regularizer': 9.705524802098772e-05, 'learning_rate': 0.00022315887005935426, 'dropout': 0.5356312187232736, 'threshold': 0.30636787388249}\n",
      "model#:  1\n",
      "performance:  [0.49386538863182067, 0.9132558941841126, 0.7860000133514404, 0.60801260471344]\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 52, 'kernel_regularizer': 2.0561657151544554e-05, 'learning_rate': 0.0004334924000007298, 'dropout': 0.13257784552647578, 'threshold': 0.964929234427516}\n",
      "model#:  2\n",
      "performance:  [0.9860644698143005, 0.9181377530097962, 0.8180000066757203, 0.9044602036476135]\n",
      "parameter_set:  {'n_hidden': 3, 'n_units': 39, 'kernel_regularizer': 0.00044140545887066046, 'learning_rate': 0.00014506328714941635, 'dropout': 0.27487045059901277, 'threshold': 0.2711898939392464}\n",
      "model#:  6\n",
      "performance:  [0.8480913639068604, 0.7416256546974183, 0.5960000157356262, 0.987744677066803]\n",
      "parameter_set:  {'n_hidden': 6, 'n_units': 16, 'kernel_regularizer': 0.005323129630596556, 'learning_rate': 0.001860641301921893, 'dropout': 0.5360667562612894, 'threshold': 0.0391717453891961}\n",
      "model#:  7\n",
      "performance:  [1.7531346797943115, 0.9098804473876954, 0.8360000014305115, 0.7495539903640747]\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 54, 'kernel_regularizer': 0.03503638181673643, 'learning_rate': 0.004905673394382963, 'dropout': 0.30586037212024797, 'threshold': 0.5940713473350903}\n",
      "model#:  18\n",
      "performance:  [2.229206371307373, 0.8609070420265198, 0.8300000071525574, 0.8576863765716553]\n",
      "parameter_set:  {'n_hidden': 6, 'n_units': 42, 'kernel_regularizer': 3.490831256354644e-05, 'learning_rate': 0.005613624226165328, 'dropout': 0.3451125268884448, 'threshold': 0.2112451732742715}\n"
     ]
    }
   ],
   "source": [
    "for i in study.best_trials:\n",
    "    print('model#: ', i.number)\n",
    "    print('performance: ', i.values)\n",
    "    print('parameter_set: ', i.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter_set:  {'n_hidden': 1, 'n_units': 57, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.0003863053283527932, 'dropout': 0.4202520981765924, 'threshold': 0.7665980174820468}\n",
      "parameter_set:  {'n_hidden': 5, 'n_units': 38, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.0005411428551177511, 'dropout': 0.052179566680561985, 'threshold': 0.5437192158028702}\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 22, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.0003863053283527932, 'dropout': 0.052179566680561985, 'threshold': 0.7665980174820468}\n",
      "parameter_set:  {'n_hidden': 2, 'n_units': 57, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.00014624928787350703, 'dropout': 0.2951334265681061, 'threshold': 0.27264665452348325}\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 38, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.0005411428551177511, 'dropout': 0.7487590993745062, 'threshold': 0.5673212758550799}\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 34, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.0005411428551177511, 'dropout': 0.3131142986244344, 'threshold': 0.48727758547438627}\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 22, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.0005411428551177511, 'dropout': 0.5746197524258702, 'threshold': 0.48727758547438627}\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 46, 'activation': 'relu', 'kernel_regularizer': 0.00014856612759129784, 'learning_rate': 0.00031121579446690323, 'dropout': 0.5534683355371559, 'threshold': 0.520033833288477}\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 22, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.0005411428551177511, 'dropout': 0.052179566680561985, 'threshold': 0.7665980174820468}\n",
      "parameter_set:  {'n_hidden': 1, 'n_units': 57, 'activation': 'relu', 'kernel_regularizer': 1.358415564119472e-05, 'learning_rate': 0.0003863053283527932, 'dropout': 0.4202520981765924, 'threshold': 0.7665980174820468}\n"
     ]
    }
   ],
   "source": [
    "for i in study.best_trials:\n",
    "    #print('model#: ', i.number)\n",
    "    #print('performance: ', i.values)\n",
    "    print('parameter_set: ', i.params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model evaluation\n",
    "\n",
    "## Generate the training/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the unseen test for final testing of the models\n",
    "test = pd.read_json('data/unseen_test.json')\n",
    "\n",
    "if 'doi' in test.columns:\n",
    "    test = test.drop(columns='doi')\n",
    "\n",
    "test = test.copy()\n",
    "test['abstract'] = test['abstract'].apply(denoise_text)\n",
    "test['abstract'] = test['abstract'].apply(remove_special_characters)\n",
    "test['abstract'] = test['abstract'].apply(normalize, lowercase=True, remove_stopwords=True)\n",
    "test['abstract'] = test['abstract'].apply(remove_punct_and_short_words)\n",
    "\n",
    "train = cleaned_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into chunks\n",
    "training_chunk1 = cleaned_df[:100]\n",
    "training_chunk2 = cleaned_df[100:200]\n",
    "training_chunk3 = cleaned_df[200:300]\n",
    "training_chunk4 = cleaned_df[300:400]\n",
    "training_chunk5 = cleaned_df[400:]\n",
    "\n",
    "# create the five different training sets                                                                             # excluded\n",
    "set1 = pd.concat([training_chunk1,training_chunk2,training_chunk3,training_chunk4]).reset_index(drop=True)            # training_chunk5\n",
    "set2 = pd.concat([training_chunk1,training_chunk2,training_chunk3,training_chunk5]).reset_index(drop=True)            # training_chunk4\n",
    "set3 = pd.concat([training_chunk1,training_chunk2,training_chunk4,training_chunk5]).reset_index(drop=True)            # training_chunk3\n",
    "set4 = pd.concat([training_chunk1,training_chunk3,training_chunk4,training_chunk5]).reset_index(drop=True)            # training_chunk2\n",
    "set5 = pd.concat([training_chunk2,training_chunk3,training_chunk4,training_chunk5]).reset_index(drop=True)            # training_chunk1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Classical models\n",
    "\n",
    "### 4.1.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict1 = {}\n",
    "\n",
    "\n",
    "for c, gamma in zip([100], [0.0001]):\n",
    "    \n",
    "    svm_res_dict = {'c':float, 'gamma':float, 'metrics': {}}\n",
    "    single_res = {'train_accuracy': float, 'test_accuracy': float, 'train_recall': float, 'test_recall': float, 'train_prec': float,\n",
    "                  'test_prec': float,'train_auc': float, 'test_auc': float, 'train_f1_pos':float, 'test_f1_pos': float, 'train_f1_neg': float, 'test_f1_neg': float}\n",
    "    \n",
    "    train_accuracy = []; test_accuracy = []; train_recall = []; test_recall = []; train_prec = []; test_prec = []; train_auc = []; test_auc = []\n",
    "    train_f1_pos = []; test_f1_pos = []; train_f1_neg = []; test_f1_neg = []\n",
    "\n",
    "    X_train_ = vec.transform(train['abstract'].values).toarray(); y_train_ = train['label'].values\n",
    "    X_test_ = vec.transform(test['abstract'].values).toarray(); y_test_ = test['label'].values\n",
    "\n",
    "\n",
    "    clf1 = SVC(C = c, gamma=gamma)\n",
    "    clf1.fit(X_train_, y_train_)\n",
    "\n",
    "    # get the rest of the metrics \n",
    "    # do this by dictionary can decrease the amount of code-lines significantly\n",
    "    \n",
    "    train_accuracy.append(accuracy_score(y_train_, clf1.predict(X_train_)))\n",
    "    test_accuracy.append(accuracy_score(y_test_, clf1.predict(X_test_)))\n",
    "\n",
    "    train_recall.append(recall_score(y_train_, clf1.predict(X_train_)))\n",
    "    test_recall.append(recall_score(y_test_, clf1.predict(X_test_)))\n",
    "\n",
    "    train_prec.append(precision_score(y_train_, clf1.predict(X_train_)))\n",
    "    test_prec.append(precision_score(y_test_, clf1.predict(X_test_)))\n",
    "\n",
    "    train_auc.append(roc_auc_score(y_train_, clf1.predict(X_train_)))\n",
    "    test_auc.append(roc_auc_score(y_test_, clf1.predict(X_test_)))\n",
    "\n",
    "    train_f1_pos.append(f1_score(y_train_, clf1.predict(X_train_)))\n",
    "    test_f1_pos.append(f1_score(y_test_, clf1.predict(X_test_)))\n",
    "\n",
    "    train_f1_neg.append(f1_score(y_train_, clf1.predict(X_train_), pos_label=0))\n",
    "    test_f1_neg.append(f1_score(y_test_, clf1.predict(X_test_), pos_label=0))\n",
    "        \n",
    "    for name, values in zip(['train_accuracy', 'test_accuracy', 'train_recall', 'test_recall', 'train_prec', 'test_prec', 'train_auc', 'test_auc', 'train_f1_pos', 'test_f1_pos', 'train_f1_neg', 'test_f1_neg'],\n",
    "                            [train_accuracy, test_accuracy, train_recall, test_recall, train_prec, test_prec, train_auc, test_auc, train_f1_pos, test_f1_pos, train_f1_neg, test_f1_neg]):\n",
    "        \n",
    "        array = np.array(values)\n",
    "        single_res[name] = np.round(array.mean(), decimals=4)\n",
    "\n",
    "        \n",
    "    svm_res_dict['c'] = c\n",
    "    svm_res_dict['gamma'] = gamma\n",
    "    svm_res_dict['metrics'] = single_res\n",
    "    \n",
    "    res_dict1['1'] = svm_res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test_, clf1.predict(X_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_prec</th>\n",
       "      <th>test_prec</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_f1_pos</th>\n",
       "      <th>test_f1_pos</th>\n",
       "      <th>train_f1_neg</th>\n",
       "      <th>test_f1_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hyper_set</th>\n",
       "      <th>C</th>\n",
       "      <th>gamma</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>0.0001</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.9714</td>\n",
       "      <td>0.9412</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>0.9412</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.9412</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>0.9388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train_accuracy  test_accuracy  train_recall  \\\n",
       "hyper_set C   gamma                                                 \n",
       "1         100 0.0001           0.978           0.94        0.9714   \n",
       "\n",
       "                      test_recall  train_prec  test_prec  train_auc  test_auc  \\\n",
       "hyper_set C   gamma                                                             \n",
       "1         100 0.0001       0.9412      0.9835     0.9412     0.9779      0.94   \n",
       "\n",
       "                      train_f1_pos  test_f1_pos  train_f1_neg  test_f1_neg  \n",
       "hyper_set C   gamma                                                         \n",
       "1         100 0.0001        0.9774       0.9412        0.9786       0.9388  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_list = []\n",
    "data_list = []\n",
    "\n",
    "for j, res in res_dict1.items():\n",
    "    \n",
    "    \n",
    "        \n",
    "    hyper_list.append((j, res['c'], res['gamma']))\n",
    "\n",
    "    data_list.append(res['metrics'])\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(hyper_list, names=['hyper_set', 'C', 'gamma'])\n",
    "svm_best_df = pd.DataFrame(data_list, index=index)\n",
    "svm_best_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need shuffling? check the \n",
    "\n",
    "res_dict2 = {}\n",
    "\n",
    "for c, solver in zip([0.1], ['lbfgs']):\n",
    "    \n",
    "    lr_res_dict = {'c':float, 'solver':str, 'metrics': {}}\n",
    "    single_res = {'train_accuracy': float, 'test_accuracy': float, 'train_recall': float, 'test_recall': float, 'train_prec': float,\n",
    "                  'test_prec': float,'train_auc': float, 'test_auc': float, 'train_f1_pos':float, 'test_f1_pos': float, 'train_f1_neg': float, 'test_f1_neg': float}\n",
    "    \n",
    "    train_accuracy = []; test_accuracy = []; train_recall = []; test_recall = []; train_prec = []; test_prec = []; train_auc = []; test_auc = []\n",
    "    train_f1_pos = []; test_f1_pos = []; train_f1_neg = []; test_f1_neg = []\n",
    "\n",
    "    X_train_ = vec.transform(train['abstract'].values).toarray(); y_train_ = train['label'].values\n",
    "    X_test_ = vec.transform(test['abstract'].values).toarray(); y_test_ = test['label'].values\n",
    "\n",
    "\n",
    "    clf2 = LogisticRegression(C=c, solver=solver)\n",
    "    clf2.fit(X_train_, y_train_)\n",
    "\n",
    "    # get the rest of the metrics\n",
    "    # do this by dictionary can decrease the amount of code-lines significantly\n",
    "    \n",
    "    train_accuracy.append(accuracy_score(y_train_, clf2.predict(X_train_)))\n",
    "    test_accuracy.append(accuracy_score(y_test_, clf2.predict(X_test_)))\n",
    "\n",
    "    train_recall.append(recall_score(y_train_, clf2.predict(X_train_)))\n",
    "    test_recall.append(recall_score(y_test_, clf2.predict(X_test_)))\n",
    "\n",
    "    train_prec.append(precision_score(y_train_, clf2.predict(X_train_)))\n",
    "    test_prec.append(precision_score(y_test_, clf2.predict(X_test_)))\n",
    "\n",
    "    train_auc.append(roc_auc_score(y_train_, clf2.predict(X_train_)))\n",
    "    test_auc.append(roc_auc_score(y_test_, clf2.predict(X_test_)))\n",
    "\n",
    "    train_f1_pos.append(f1_score(y_train_, clf2.predict(X_train_)))\n",
    "    test_f1_pos.append(f1_score(y_test_, clf2.predict(X_test_)))\n",
    "\n",
    "    train_f1_neg.append(f1_score(y_train_, clf2.predict(X_train_), pos_label=0))\n",
    "    test_f1_neg.append(f1_score(y_test_, clf2.predict(X_test_), pos_label=0))\n",
    "    \n",
    "    for name, values in zip(['train_accuracy', 'test_accuracy', 'train_recall', 'test_recall', 'train_prec', 'test_prec', 'train_auc', 'test_auc', 'train_f1_pos', 'test_f1_pos', 'train_f1_neg', 'test_f1_neg'],\n",
    "                            [train_accuracy, test_accuracy, train_recall, test_recall, train_prec, test_prec, train_auc, test_auc, train_f1_pos, test_f1_pos, train_f1_neg, test_f1_neg]):\n",
    "        \n",
    "        array = np.array(values)\n",
    "        single_res[name] = np.round(array.mean(), decimals=3)\n",
    "        \n",
    "        \n",
    "    lr_res_dict['c'] = c\n",
    "    lr_res_dict['solver'] = solver\n",
    "    lr_res_dict['metrics'] = single_res\n",
    "    \n",
    "    res_dict2['1'] = lr_res_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_prec</th>\n",
       "      <th>test_prec</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_f1_pos</th>\n",
       "      <th>test_f1_pos</th>\n",
       "      <th>train_f1_neg</th>\n",
       "      <th>test_f1_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hyper_set</th>\n",
       "      <th>C</th>\n",
       "      <th>solver</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>lbfgs</th>\n",
       "      <td>0.984</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train_accuracy  test_accuracy  train_recall  \\\n",
       "hyper_set C   solver                                                \n",
       "1         0.1 lbfgs            0.984           0.93          0.98   \n",
       "\n",
       "                      test_recall  train_prec  test_prec  train_auc  test_auc  \\\n",
       "hyper_set C   solver                                                            \n",
       "1         0.1 lbfgs         0.941       0.988      0.923      0.984      0.93   \n",
       "\n",
       "                      train_f1_pos  test_f1_pos  train_f1_neg  test_f1_neg  \n",
       "hyper_set C   solver                                                        \n",
       "1         0.1 lbfgs          0.984        0.932         0.984        0.928  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_list1 = []\n",
    "data_list1 = []\n",
    "\n",
    "for j, res in res_dict2.items():\n",
    "    \n",
    "    \n",
    "        \n",
    "    hyper_list1.append((j, res['c'], res['solver']))\n",
    "\n",
    "    data_list1.append(res['metrics'])\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(hyper_list1, names=['hyper_set', 'C', 'solver'])\n",
    "lr_best_df = pd.DataFrame(data_list1, index=index)\n",
    "lr_best_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict3 = {}\n",
    "\n",
    "for n_estimators, criterion in zip([1000], ['entropy']):\n",
    "    \n",
    "    rf_res_dict = {'n_estimators':int, 'criterion':str, 'metrics': {}}\n",
    "    single_res = {'train_accuracy': float, 'test_accuracy': float, 'train_recall': float, 'test_recall': float, 'train_prec': float,\n",
    "                  'test_prec': float,'train_auc': float, 'test_auc': float, 'train_f1_pos':float, 'test_f1_pos': float, 'train_f1_neg': float, 'test_f1_neg': float}\n",
    "    \n",
    "    train_accuracy = []; test_accuracy = []; train_recall = []; test_recall = []; train_prec = []; test_prec = []; train_auc = []; test_auc = []\n",
    "    train_f1_pos = []; test_f1_pos = []; train_f1_neg = []; test_f1_neg = []\n",
    "    \n",
    "    X_train_ = vec.transform(train['abstract'].values).toarray(); y_train_ = train['label'].values\n",
    "    X_test_ = vec.transform(test['abstract'].values).toarray(); y_test_ = test['label'].values\n",
    "\n",
    "\n",
    "    clf3 = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion)\n",
    "    clf3.fit(X_train_, y_train_)\n",
    "\n",
    "    # get the rest of the metrics\n",
    "    # do this by dictionary can decrease the amount of code-lines significantly\n",
    "    \n",
    "    train_accuracy.append(accuracy_score(y_train_, clf3.predict(X_train_)))\n",
    "    test_accuracy.append(accuracy_score(y_test_, clf3.predict(X_test_)))\n",
    "\n",
    "    train_recall.append(recall_score(y_train_, clf3.predict(X_train_)))\n",
    "    test_recall.append(recall_score(y_test_, clf3.predict(X_test_)))\n",
    "\n",
    "    train_prec.append(precision_score(y_train_, clf3.predict(X_train_)))\n",
    "    test_prec.append(precision_score(y_test_, clf3.predict(X_test_)))\n",
    "\n",
    "    train_auc.append(roc_auc_score(y_train_, clf3.predict(X_train_)))\n",
    "    test_auc.append(roc_auc_score(y_test_, clf3.predict(X_test_)))\n",
    "\n",
    "    train_f1_pos.append(f1_score(y_train_, clf3.predict(X_train_)))\n",
    "    test_f1_pos.append(f1_score(y_test_, clf3.predict(X_test_)))\n",
    "\n",
    "    train_f1_neg.append(f1_score(y_train_, clf3.predict(X_train_), pos_label=0))\n",
    "    test_f1_neg.append(f1_score(y_test_, clf3.predict(X_test_), pos_label=0))\n",
    "    \n",
    "    for name, values in zip(['train_accuracy', 'test_accuracy', 'train_recall', 'test_recall', 'train_prec', 'test_prec', 'train_auc', 'test_auc', 'train_f1_pos', 'test_f1_pos', 'train_f1_neg', 'test_f1_neg'],\n",
    "                            [train_accuracy, test_accuracy, train_recall, test_recall, train_prec, test_prec, train_auc, test_auc, train_f1_pos, test_f1_pos, train_f1_neg, test_f1_neg]):\n",
    "        \n",
    "        array = np.array(values)\n",
    "        single_res[name] = np.round(array.mean(), decimals=3)\n",
    "        \n",
    "        \n",
    "    rf_res_dict['n_estimators'] = n_estimators\n",
    "    rf_res_dict['criterion'] = criterion\n",
    "    rf_res_dict['metrics'] = single_res\n",
    "    \n",
    "    res_dict3['1'] = rf_res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_prec</th>\n",
       "      <th>test_prec</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_f1_pos</th>\n",
       "      <th>test_f1_pos</th>\n",
       "      <th>train_f1_neg</th>\n",
       "      <th>test_f1_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hyper_set</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>criterion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1000</th>\n",
       "      <th>entropy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  train_accuracy  test_accuracy  train_recall  \\\n",
       "hyper_set n_estimators criterion                                                \n",
       "1         1000         entropy               1.0           0.95           1.0   \n",
       "\n",
       "                                  test_recall  train_prec  test_prec  \\\n",
       "hyper_set n_estimators criterion                                       \n",
       "1         1000         entropy           0.98         1.0      0.926   \n",
       "\n",
       "                                  train_auc  test_auc  train_f1_pos  \\\n",
       "hyper_set n_estimators criterion                                      \n",
       "1         1000         entropy          1.0     0.949           1.0   \n",
       "\n",
       "                                  test_f1_pos  train_f1_neg  test_f1_neg  \n",
       "hyper_set n_estimators criterion                                          \n",
       "1         1000         entropy          0.952           1.0        0.947  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_list = []\n",
    "data_list = []\n",
    "\n",
    "for j, res in res_dict3.items():\n",
    "    \n",
    "    \n",
    "        \n",
    "    hyper_list.append((j, res['n_estimators'], res['criterion']))\n",
    "\n",
    "    data_list.append(res['metrics'])\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(hyper_list, names=['hyper_set', 'n_estimators', 'criterion'])\n",
    "rf_best_df = pd.DataFrame(data_list, index=index)\n",
    "rf_best_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ann_model(n_hidden, n_units, activation, kernel_regularizer, lr, do, threshold):\n",
    "    \n",
    "    \n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape = (1000,)))\n",
    "\n",
    "    for i in range(n_hidden):\n",
    "        \n",
    "        model.add(keras.layers.Dense(n_units, activation=activation, kernel_regularizer=regularizers.L2(kernel_regularizer)))\n",
    "\n",
    "    model.add(keras.layers.Dropout(do))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    \n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate=lr),\n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=['AUC',\n",
    "                            keras.metrics.BinaryAccuracy(threshold=threshold),\n",
    "                            keras.metrics.Recall(thresholds=threshold),\n",
    "                            keras.metrics.Precision(thresholds=threshold)])\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "# parametersets\n",
    "# 1. {'vector_size': 95, 'context_window': 9, 'min_count': 4, 'epochs': 8, 'dim': 224, 'padding': 'pre', 'learning_rate': 0.013121832123684417, 'dropout': 0.4402980193837679, 'threshold': 0.5099871650242483}\n",
    "# 2. {'vector_size': 98, 'context_window': 8, 'min_count': 4, 'epochs': 7, 'dim': 217, 'padding': 'pre', 'learning_rate': 0.002104272243300588, 'dropout': 0.5665904244287674, 'threshold': 0.4837522534769876}\n",
    "\n",
    "\n",
    "def test_models(n_hidden, n_units, activation, kernel_regularizer, learning_rate, dropout, threshold):\n",
    "\n",
    "    res_dict = {}; temp_dict = {}\n",
    "    train_dict = {}; test_dict = {}\n",
    "\n",
    "    tr_loss_list = []; tr_auc_list = []; tr_prec_list = []; tr_acc_list = []; tr_rec_list = []; tr_fpos_list = []; tr_fneg_list = []\n",
    "    te_loss_list = []; te_auc_list = []; te_prec_list = []; te_acc_list = []; te_rec_list = []; te_fpos_list = []; te_fneg_list = []\n",
    "    \n",
    "    X = vec.transform(train['abstract'].values).toarray(); y = train['label'].values\n",
    "    X_test = vec.transform(test['abstract'].values).toarray(); y_test = test['label'].values\n",
    "\n",
    "    for rs in range(10):\n",
    "        \n",
    "        ann_model = get_ann_model(n_hidden = n_hidden,\n",
    "                                    n_units = n_units,\n",
    "                                    activation = activation,\n",
    "                                    kernel_regularizer = kernel_regularizer,\n",
    "                                    lr=learning_rate, \n",
    "                                    do=dropout, \n",
    "                                    threshold=threshold)\n",
    "        \n",
    "        \n",
    "        ann_model.fit(X, y, validation_split=0.25, verbose=0, epochs=100, batch_size=100)\n",
    "\n",
    "        tr_score = ann_model.evaluate(X, y, verbose=0)\n",
    "        te_score = ann_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "        ################# Manually handle the f1-score #####################\n",
    "        # train\n",
    "        pred = ann_model.predict(X, verbose=0) # y is true\n",
    "        f1_train_pos = calculate_f1_score(y, pred, threshold=threshold, pos_label=1)\n",
    "        f1_trian_neg = calculate_f1_score(y, pred, threshold=threshold, pos_label=0)\n",
    "\n",
    "        # test\n",
    "        test_pred = ann_model.predict(X_test, verbose=0)\n",
    "        f1_test_pos = calculate_f1_score(y_test, test_pred, threshold=threshold, pos_label=1)\n",
    "        f1_test_neg = calculate_f1_score(y_test, test_pred, threshold=threshold, pos_label=0)\n",
    "        \n",
    "        # save the f1-scores\n",
    "        tr_fpos_list.append(f1_train_pos); tr_fneg_list.append(f1_trian_neg)\n",
    "        te_fpos_list.append(f1_test_pos); te_fneg_list.append(f1_test_neg)\n",
    "        \n",
    "        tr_loss_list.append(tr_score[0]); tr_auc_list.append(tr_score[1]);  tr_acc_list.append(tr_score[2]); tr_rec_list.append(tr_score[3]); tr_prec_list.append(tr_score[4])\n",
    "        te_loss_list.append(te_score[0]); te_auc_list.append(te_score[1]);  te_acc_list.append(te_score[2]); te_rec_list.append(te_score[3]); te_prec_list.append(te_score[4])\n",
    "        \n",
    "    train_dict['loss'] = tr_loss_list; train_dict['auc'] = tr_auc_list; train_dict['prec'] = tr_prec_list; train_dict['acc'] = tr_acc_list; train_dict['rec'] = tr_rec_list; train_dict['f1pos'] = tr_fpos_list; train_dict['f1neg'] = tr_fneg_list\n",
    "    test_dict['loss'] = te_loss_list; test_dict['auc'] = te_auc_list; test_dict['prec'] = te_prec_list; test_dict['acc'] = te_acc_list; test_dict['rec'] = te_rec_list; test_dict['f1pos'] = te_fpos_list; test_dict['f1neg'] = te_fneg_list\n",
    "    \n",
    "\n",
    "    temp_dict['train'] = train_dict\n",
    "    temp_dict['test'] = test_dict\n",
    "\n",
    "    \n",
    "        \n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555} ,\n",
      "{'n_hidden': 1, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0005834382440750414, 'dropout': 0.48787589688314237, 'threshold': 0.7622802703643394} ,\n",
      "{'n_hidden': 10, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.6376447344086704, 'threshold': 0.2292333004984978} ,\n",
      "{'n_hidden': 1, 'n_units': 37, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0006521686023472134, 'dropout': 0.2862399358316623, 'threshold': 0.8171312422835556} ,\n",
      "{'n_hidden': 7, 'n_units': 6, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0013414232848841775, 'dropout': 0.6061345542965496, 'threshold': 0.3544065415833555} ,\n",
      "{'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 0.00032005478454991034, 'learning_rate': 8.446138571374139e-05, 'dropout': 0.8630165121896143, 'threshold': 0.52263525360727} ,\n",
      "{'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.3395203701483684, 'threshold': 0.3544065415833555} ,\n",
      "{'n_hidden': 9, 'n_units': 54, 'kernel_regularizer': 2.0678394943630145e-05, 'learning_rate': 6.444708842918975e-05, 'dropout': 0.48726918436401034, 'threshold': 0.7581930049210841} ,\n",
      "{'n_hidden': 3, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0005834382440750414, 'dropout': 0.8662788901341136, 'threshold': 0.3056244326515286} ,\n",
      "{'n_hidden': 1, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.12198972576711176, 'dropout': 0.6376447344086704, 'threshold': 0.3318972015983154} ,\n",
      "{'n_hidden': 1, 'n_units': 33, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.1697866286480649, 'threshold': 0.48870052508813955} ,\n",
      "{'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 0.019189432566192953, 'learning_rate': 0.0007970799285309584, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114} ,\n",
      "{'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 0.00023198013313174801, 'learning_rate': 7.015653993460166e-05, 'dropout': 0.11174948366441749, 'threshold': 0.31665717730358856} ,\n",
      "{'n_hidden': 1, 'n_units': 37, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0006521686023472134, 'dropout': 0.048525896101412724, 'threshold': 0.27265874826006264} ,\n",
      "{'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.3923850775943305} ,\n",
      "{'n_hidden': 2, 'n_units': 11, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 0.00039626071240343367, 'dropout': 0.8568014240464407, 'threshold': 0.10873515186137227} ,\n",
      "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555} ,\n",
      "{'n_hidden': 1, 'n_units': 6, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.6061345542965496, 'threshold': 0.3318972015983154} ,\n",
      "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.48870052508813955} ,\n",
      "{'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 0.0005069808946385528, 'learning_rate': 0.021296428950781383, 'dropout': 0.8662788901341136, 'threshold': 0.3544065415833555} ,\n",
      "{'n_hidden': 9, 'n_units': 8, 'kernel_regularizer': 0.0004416315616331735, 'learning_rate': 0.00020655775452336643, 'dropout': 0.5039281376641517, 'threshold': 0.4195383350653711} ,\n",
      "{'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 7.015653993460166e-05, 'dropout': 0.7860688156703391, 'threshold': 0.1325102969894017} ,\n",
      "{'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 1.8148712421091602e-05, 'learning_rate': 0.002389600411589469, 'dropout': 0.48787589688314237, 'threshold': 0.3814482156016251} ,\n",
      "{'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 9.988480575396382e-05, 'dropout': 0.5499841382958144, 'threshold': 0.27265874826006264} ,\n",
      "{'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 0.019189432566192953, 'learning_rate': 0.0007970799285309584, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114} ,\n",
      "{'n_hidden': 1, 'n_units': 64, 'kernel_regularizer': 0.0007793010643204281, 'learning_rate': 0.0003795051194324455, 'dropout': 0.7010918993652716, 'threshold': 0.3056244326515286} ,\n",
      "{'n_hidden': 1, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.1325102969894017} ,\n",
      "{'n_hidden': 7, 'n_units': 25, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 9.988480575396382e-05, 'dropout': 0.5499841382958144, 'threshold': 0.27265874826006264} ,\n",
      "{'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.3923850775943305} ,\n",
      "{'n_hidden': 3, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 5.3577811386515144e-05, 'dropout': 0.6571075663804247, 'threshold': 0.3056244326515286} ,\n",
      "{'n_hidden': 7, 'n_units': 14, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0006955292277375732, 'dropout': 0.8959560597731027, 'threshold': 0.3544065415833555} ,\n",
      "{'n_hidden': 3, 'n_units': 56, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.0012602416730026099, 'dropout': 0.0686413637905968, 'threshold': 0.4195383350653711} ,\n",
      "{'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 0.0017899630867169573, 'learning_rate': 0.0005834382440750414, 'dropout': 0.7860688156703391, 'threshold': 0.7347123009931876} ,\n",
      "{'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.6061345542965496, 'threshold': 0.1325102969894017} ,\n",
      "{'n_hidden': 5, 'n_units': 33, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.001598284971321218, 'dropout': 0.20902702870023096, 'threshold': 0.3544065415833555} ,\n",
      "{'n_hidden': 10, 'n_units': 44, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.00012248524389842045, 'dropout': 0.3395203701483684, 'threshold': 0.8929033947032199} ,\n",
      "{'n_hidden': 2, 'n_units': 42, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 4.9567167614517136e-05, 'dropout': 0.08891052102456992, 'threshold': 0.2292333004984978} ,\n",
      "{'n_hidden': 2, 'n_units': 44, 'kernel_regularizer': 1.1874693702272719e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.8929033947032199} ,\n",
      "{'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 0.12254567223517063, 'learning_rate': 0.003763855754353628, 'dropout': 0.8662788901341136, 'threshold': 0.3544065415833555} ,\n",
      "{'n_hidden': 1, 'n_units': 58, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0003795051194324455, 'dropout': 0.20896209008607705, 'threshold': 0.383494481615293} ,\n",
      "{'n_hidden': 7, 'n_units': 14, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 4.9567167614517136e-05, 'dropout': 0.545499225702559, 'threshold': 0.2292333004984978} ,\n"
     ]
    }
   ],
   "source": [
    "# filtering, to get the best among the stores \"best_trials\"\n",
    "\n",
    "for i in study.best_trials:\n",
    "    #print(i.params, ',')\n",
    "    if i.values[0] > 0.80:\n",
    "        if i.values[1] > 0.80:\n",
    "            if i.values[2] > 0.80:\n",
    "                if i.values[3] > 0.80:\n",
    "                    #print(i.values)\n",
    "                    print(i.params, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_para_set {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58, 'activation': 'relu'}\n",
      "{'train': {'loss': [0.18023273348808289, 0.2054823637008667, 0.1593029499053955, 0.1882626861333847, 0.17200042307376862, 0.18379145860671997, 0.17650265991687775, 0.16214197874069214, 0.20090630650520325, 0.1741788238286972], 'auc': [0.9895238876342773, 0.9814726114273071, 0.9923569560050964, 0.9848579168319702, 0.9895398616790771, 0.9850980043411255, 0.9871628880500793, 0.9904442429542542, 0.9867626428604126, 0.9893397092819214], 'prec': [0.9702127575874329, 0.949999988079071, 0.9714285731315613, 0.9666666388511658, 0.9628099203109741, 0.9539749026298523, 0.9708333611488342, 0.9435483813285828, 0.9502074718475342, 0.9629629850387573], 'acc': [0.9520000219345093, 0.9419999718666077, 0.972000002861023, 0.9580000042915344, 0.9580000042915344, 0.9440000057220459, 0.9620000123977661, 0.949999988079071, 0.9440000057220459, 0.9599999785423279], 'rec': [0.9306122660636902, 0.9306122660636902, 0.9714285731315613, 0.9469387531280518, 0.9510204195976257, 0.9306122660636902, 0.9510204195976257, 0.9551020264625549, 0.9346938729286194, 0.9551020264625549], 'f1pos': [0.95, 0.9402061855670103, 0.9714285714285714, 0.9567010309278351, 0.9568788501026693, 0.9421487603305784, 0.9608247422680413, 0.9492900608519269, 0.9423868312757202, 0.959016393442623], 'f1neg': [0.9538461538461539, 0.9436893203883495, 0.9725490196078431, 0.9592233009708738, 0.9590643274853802, 0.9457364341085271, 0.9631067961165048, 0.9506903353057199, 0.9455252918287939, 0.9609375]}, 'test': {'loss': [0.2909501791000366, 0.35855552554130554, 0.34192752838134766, 0.30808794498443604, 0.31217870116233826, 0.2861683964729309, 0.3984144628047943, 0.38186004757881165, 0.33854958415031433, 0.36111828684806824], 'auc': [0.9671868085861206, 0.9355742335319519, 0.9305722117424011, 0.9511804580688477, 0.9473789930343628, 0.9671868681907654, 0.9205682277679443, 0.9121648073196411, 0.9465786218643188, 0.9359743595123291], 'prec': [0.914893627166748, 0.8799999952316284, 0.9111111164093018, 0.918367326259613, 0.9200000166893005, 0.8823529481887817, 0.8181818127632141, 0.9166666865348816, 0.9333333373069763, 0.8627451062202454], 'acc': [0.8799999952316284, 0.8700000047683716, 0.8600000143051147, 0.8999999761581421, 0.9100000262260437, 0.8799999952316284, 0.8399999737739563, 0.8899999856948853, 0.8799999952316284, 0.8600000143051147], 'rec': [0.843137264251709, 0.8627451062202454, 0.8039215803146362, 0.8823529481887817, 0.9019607901573181, 0.8823529481887817, 0.8823529481887817, 0.8627451062202454, 0.8235294222831726, 0.8627451062202454], 'f1pos': [0.8775510204081632, 0.8712871287128714, 0.8541666666666666, 0.9, 0.9108910891089109, 0.8823529411764706, 0.8490566037735848, 0.888888888888889, 0.8749999999999999, 0.8627450980392157], 'f1neg': [0.8823529411764707, 0.8686868686868686, 0.8653846153846154, 0.9, 0.9090909090909091, 0.8775510204081631, 0.8297872340425533, 0.8910891089108911, 0.8846153846153846, 0.8571428571428571]}}\n",
      "current_para_set {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-06, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58, 'activation': 'relu'}\n",
      "{'train': {'loss': [0.15893526375293732, 0.18090054392814636, 0.18400387465953827, 0.17460909485816956, 0.1518281102180481, 0.16500164568424225, 0.22864359617233276, 0.18979869782924652, 0.13099586963653564, 0.1677357256412506], 'auc': [0.9929171204566956, 0.990476131439209, 0.987747073173523, 0.9897238612174988, 0.992901086807251, 0.9912844896316528, 0.9804481863975525, 0.9875230193138123, 0.994157612323761, 0.987739086151123], 'prec': [0.9586777091026306, 0.9745762944221497, 0.9702127575874329, 0.9624999761581421, 0.9554656147956848, 0.9508196711540222, 0.949999988079071, 0.9541666507720947, 0.9669421315193176, 0.9628099203109741], 'acc': [0.9539999961853027, 0.9580000042915344, 0.9520000219345093, 0.9539999961853027, 0.9599999785423279, 0.949999988079071, 0.9419999718666077, 0.9459999799728394, 0.9620000123977661, 0.9580000042915344], 'rec': [0.9469387531280518, 0.9387755393981934, 0.9306122660636902, 0.9428571462631226, 0.9632652997970581, 0.9469387531280518, 0.9306122660636902, 0.9346938729286194, 0.9551020264625549, 0.9510204195976257], 'f1pos': [0.9527720739219712, 0.9563409563409563, 0.95, 0.9525773195876288, 0.959349593495935, 0.9488752556237218, 0.9402061855670103, 0.9443298969072165, 0.9609856262833676, 0.9568788501026693], 'f1neg': [0.9551656920077972, 0.9595375722543352, 0.9538461538461539, 0.9553398058252427, 0.9606299212598426, 0.9510763209393347, 0.9436893203883495, 0.9475728155339807, 0.962962962962963, 0.9590643274853802]}, 'test': {'loss': [0.31626787781715393, 0.30352166295051575, 0.30784183740615845, 0.3336406946182251, 0.2775227129459381, 0.29427218437194824, 0.3820435702800751, 0.31951195001602173, 0.3215365707874298, 0.32841044664382935], 'auc': [0.9507802724838257, 0.9681872129440308, 0.9557823538780212, 0.9517806768417358, 0.9587835669517517, 0.9505802392959595, 0.9401760697364807, 0.9473789930343628, 0.9411765336990356, 0.9389755725860596], 'prec': [0.9166666865348816, 0.9555555582046509, 0.8727272748947144, 0.8999999761581421, 0.9166666865348816, 0.9215686321258545, 0.8653846383094788, 0.8979591727256775, 0.9130434989929199, 0.8181818127632141], 'acc': [0.8899999856948853, 0.8999999761581421, 0.8999999761581421, 0.8899999856948853, 0.8899999856948853, 0.9200000166893005, 0.8700000047683716, 0.8799999952316284, 0.8700000047683716, 0.8399999737739563], 'rec': [0.8627451062202454, 0.843137264251709, 0.9411764740943909, 0.8823529481887817, 0.8627451062202454, 0.9215686321258545, 0.8823529481887817, 0.8627451062202454, 0.8235294222831726, 0.8823529481887817], 'f1pos': [0.888888888888889, 0.8958333333333333, 0.9056603773584905, 0.8910891089108911, 0.888888888888889, 0.9215686274509803, 0.8737864077669903, 0.8799999999999999, 0.865979381443299, 0.8490566037735848], 'f1neg': [0.8910891089108911, 0.9038461538461537, 0.8936170212765957, 0.888888888888889, 0.8910891089108911, 0.9183673469387755, 0.8659793814432989, 0.8799999999999999, 0.8737864077669903, 0.8297872340425533]}}\n",
      "current_para_set {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617934e-07, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58, 'activation': 'relu'}\n",
      "{'train': {'loss': [0.176127627491951, 0.15636897087097168, 0.17831753194332123, 0.19361357390880585, 0.17883147299289703, 0.17956039309501648, 0.18118421733379364, 0.16150163114070892, 0.16580981016159058, 0.1761336475610733], 'auc': [0.9902280569076538, 0.9950140118598938, 0.9888194799423218, 0.9883473515510559, 0.9892756938934326, 0.9898679852485657, 0.9880832433700562, 0.9911884665489197, 0.9916846752166748, 0.9869228005409241], 'prec': [0.9545454382896423, 0.9750000238418579, 0.949367105960846, 0.9426229596138, 0.9508196711540222, 0.9626556038856506, 0.9665272235870361, 0.9747899174690247, 0.9409449100494385, 0.9510204195976257], 'acc': [0.949999988079071, 0.9660000205039978, 0.9359999895095825, 0.9419999718666077, 0.949999988079071, 0.9559999704360962, 0.9559999704360962, 0.9620000123977661, 0.9580000042915344, 0.9520000219345093], 'rec': [0.9428571462631226, 0.9551020264625549, 0.918367326259613, 0.9387755393981934, 0.9469387531280518, 0.9469387531280518, 0.9428571462631226, 0.9469387531280518, 0.9755101799964905, 0.9510204195976257], 'f1pos': [0.9486652977412731, 0.9649484536082473, 0.933609958506224, 0.9406952965235175, 0.9488752556237218, 0.9547325102880659, 0.9545454545454545, 0.9606625258799173, 0.9579158316633267, 0.9510204081632653], 'f1neg': [0.9512670565302145, 0.9669902912621359, 0.9382239382239382, 0.9432485322896281, 0.9510763209393347, 0.9571984435797666, 0.9573643410852712, 0.9632495164410056, 0.9580838323353293, 0.9529411764705882]}, 'test': {'loss': [0.311921089887619, 0.3012513220310211, 0.3561953008174896, 0.39241665601730347, 0.29366055130958557, 0.3430749773979187, 0.2843409776687622, 0.35005298256874084, 0.3648127317428589, 0.3507419228553772], 'auc': [0.9649859666824341, 0.952981173992157, 0.9391756653785706, 0.9285714626312256, 0.965986430644989, 0.9443777799606323, 0.9727891087532043, 0.9385754466056824, 0.9171668887138367, 0.9337735772132874], 'prec': [0.8846153616905212, 0.8799999952316284, 0.8799999952316284, 0.8679245114326477, 0.9200000166893005, 0.8703703880310059, 0.9215686321258545, 0.8979591727256775, 0.8823529481887817, 0.8679245114326477], 'acc': [0.8899999856948853, 0.8700000047683716, 0.8700000047683716, 0.8799999952316284, 0.9100000262260437, 0.8899999856948853, 0.9200000166893005, 0.8799999952316284, 0.8799999952316284, 0.8799999952316284], 'rec': [0.9019607901573181, 0.8627451062202454, 0.8627451062202454, 0.9019607901573181, 0.9019607901573181, 0.9215686321258545, 0.9215686321258545, 0.8627451062202454, 0.8823529481887817, 0.9019607901573181], 'f1pos': [0.8932038834951457, 0.8712871287128714, 0.8712871287128714, 0.8846153846153846, 0.9108910891089109, 0.8952380952380952, 0.9215686274509803, 0.8799999999999999, 0.8823529411764706, 0.8846153846153846], 'f1neg': [0.8865979381443299, 0.8686868686868686, 0.8686868686868686, 0.875, 0.9090909090909091, 0.8842105263157894, 0.9183673469387755, 0.8799999999999999, 0.8775510204081631, 0.875]}}\n",
      "current_para_set {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617934e-08, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58, 'activation': 'relu'}\n",
      "{'train': {'loss': [0.19099301099777222, 0.1592249870300293, 0.1627013385295868, 0.18110360205173492, 0.17614798247814178, 0.17834721505641937, 0.16664229333400726, 0.17581170797348022, 0.1689072549343109, 0.16213414072990417], 'auc': [0.9859383702278137, 0.9908443689346313, 0.9885634183883667, 0.9910844564437866, 0.9930051565170288, 0.9897078275680542, 0.9895797967910767, 0.9881232380867004, 0.9909324049949646, 0.9930212497711182], 'prec': [0.9469387531280518, 0.9668049812316895, 0.9365079402923584, 0.9742489457130432, 0.9516128897666931, 0.9789915680885315, 0.9547325372695923, 0.9435483813285828, 0.9628099203109741, 0.9514170289039612], 'acc': [0.9480000138282776, 0.9599999785423279, 0.949999988079071, 0.9520000219345093, 0.9580000042915344, 0.9660000205039978, 0.9520000219345093, 0.949999988079071, 0.9580000042915344, 0.9559999704360962], 'rec': [0.9469387531280518, 0.9510204195976257, 0.9632652997970581, 0.9265305995941162, 0.9632652997970581, 0.9510204195976257, 0.9469387531280518, 0.9551020264625549, 0.9510204195976257, 0.9591836929321289], 'f1pos': [0.9469387755102041, 0.9588477366255144, 0.9496981891348089, 0.9497907949790796, 0.9574036511156186, 0.9648033126293996, 0.9508196721311476, 0.9492900608519269, 0.9568788501026693, 0.9552845528455284], 'f1neg': [0.9490196078431372, 0.961089494163424, 0.9502982107355866, 0.9540229885057471, 0.9585798816568047, 0.9671179883945841, 0.953125, 0.9506903353057199, 0.9590643274853802, 0.9566929133858267]}, 'test': {'loss': [0.3824111223220825, 0.24730080366134644, 0.361234575510025, 0.3501805067062378, 0.31319865584373474, 0.3084249198436737, 0.34383174777030945, 0.3340471386909485, 0.3307635188102722, 0.3334435522556305], 'auc': [0.9219687581062317, 0.976590633392334, 0.9313725829124451, 0.9403761625289917, 0.9601840972900391, 0.960584282875061, 0.9383753538131714, 0.9465786814689636, 0.9445778727531433, 0.942376971244812], 'prec': [0.8999999761581421, 0.9591836929321289, 0.8571428656578064, 0.8571428656578064, 0.8979591727256775, 0.936170220375061, 0.8727272748947144, 0.8703703880310059, 0.8600000143051147, 0.8979591727256775], 'acc': [0.8899999856948853, 0.9399999976158142, 0.8899999856948853, 0.8399999737739563, 0.8799999952316284, 0.8999999761581421, 0.8999999761581421, 0.8899999856948853, 0.8500000238418579, 0.8799999952316284], 'rec': [0.8823529481887817, 0.9215686321258545, 0.9411764740943909, 0.8235294222831726, 0.8627451062202454, 0.8627451062202454, 0.9411764740943909, 0.9215686321258545, 0.843137264251709, 0.8627451062202454], 'f1pos': [0.8910891089108911, 0.9400000000000001, 0.897196261682243, 0.84, 0.8799999999999999, 0.8979591836734694, 0.9056603773584905, 0.8952380952380952, 0.8514851485148515, 0.8799999999999999], 'f1neg': [0.888888888888889, 0.9400000000000001, 0.8817204301075268, 0.84, 0.8799999999999999, 0.9019607843137256, 0.8936170212765957, 0.8842105263157894, 0.8484848484848485, 0.8799999999999999]}}\n",
      "current_para_set {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-09, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58, 'activation': 'relu'}\n",
      "{'train': {'loss': [0.17643728852272034, 0.18672852218151093, 0.17745183408260345, 0.19771027565002441, 0.19269190728664398, 0.1945350170135498, 0.17728354036808014, 0.1512511968612671, 0.18097776174545288, 0.17078399658203125], 'auc': [0.989091694355011, 0.9884353876113892, 0.9884674549102783, 0.9843857884407043, 0.9885634183883667, 0.9845378398895264, 0.9892196655273438, 0.9937014579772949, 0.9892997741699219, 0.9911004304885864], 'prec': [0.9508196711540222, 0.9590163826942444, 0.9426229596138, 0.9549180269241333, 0.9506173133850098, 0.9462810158729553, 0.9552845358848572, 0.970588207244873, 0.9433198571205139, 0.9670782089233398], 'acc': [0.949999988079071, 0.9580000042915344, 0.9419999718666077, 0.9539999961853027, 0.9480000138282776, 0.9419999718666077, 0.9580000042915344, 0.9580000042915344, 0.9480000138282776, 0.9639999866485596], 'rec': [0.9469387531280518, 0.9551020264625549, 0.9387755393981934, 0.9510204195976257, 0.9428571462631226, 0.9346938729286194, 0.9591836929321289, 0.9428571462631226, 0.9510204195976257, 0.9591836929321289], 'f1pos': [0.9488752556237218, 0.9570552147239264, 0.9406952965235175, 0.9529652351738241, 0.9467213114754098, 0.9404517453798767, 0.9572301425661914, 0.9565217391304348, 0.9471544715447154, 0.9631147540983606], 'f1neg': [0.9510763209393347, 0.958904109589041, 0.9432485322896281, 0.9549902152641879, 0.9492187499999999, 0.9434697855750486, 0.9587426326129665, 0.9593810444874276, 0.9488188976377954, 0.96484375]}, 'test': {'loss': [0.33234500885009766, 0.3169189393520355, 0.27105677127838135, 0.3439558148384094, 0.3492083251476288, 0.31252631545066833, 0.35645800828933716, 0.2634407877922058, 0.2893615961074829, 0.3253927528858185], 'auc': [0.9459784030914307, 0.9649860262870789, 0.971188485622406, 0.9447779059410095, 0.9579831957817078, 0.9563825726509094, 0.9489796161651611, 0.9735894203186035, 0.9597839117050171, 0.947178840637207], 'prec': [0.8799999952316284, 0.9019607901573181, 0.9200000166893005, 0.8867924809455872, 0.8620689511299133, 0.918367326259613, 0.8888888955116272, 0.930232584476471, 0.9375, 0.914893627166748], 'acc': [0.8700000047683716, 0.8999999761581421, 0.9100000262260437, 0.8999999761581421, 0.9100000262260437, 0.8999999761581421, 0.9100000262260437, 0.8600000143051147, 0.9100000262260437, 0.8799999952316284], 'rec': [0.8627451062202454, 0.9019607901573181, 0.9019607901573181, 0.9215686321258545, 0.9803921580314636, 0.8823529481887817, 0.9411764740943909, 0.7843137383460999, 0.8823529481887817, 0.843137264251709], 'f1pos': [0.8712871287128714, 0.9019607843137255, 0.9108910891089109, 0.9038461538461539, 0.9174311926605505, 0.9, 0.9142857142857143, 0.851063829787234, 0.9090909090909091, 0.8775510204081632], 'f1neg': [0.8686868686868686, 0.8979591836734694, 0.9090909090909091, 0.8958333333333333, 0.9010989010989012, 0.9, 0.9052631578947369, 0.8679245283018868, 0.9108910891089108, 0.8823529411764707]}}\n",
      "current_para_set {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617934e-10, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58, 'activation': 'relu'}\n",
      "{'train': {'loss': [0.14917944371700287, 0.2068100869655609, 0.17514269053936005, 0.17079870402812958, 0.17858672142028809, 0.20541171729564667, 0.17070922255516052, 0.1919059455394745, 0.19009403884410858, 0.1897098273038864], 'auc': [0.9934053421020508, 0.9857543706893921, 0.9910364151000977, 0.9894517660140991, 0.9893237948417664, 0.9859464168548584, 0.9895078539848328, 0.9912604689598083, 0.985626220703125, 0.9890195727348328], 'prec': [0.9663865566253662, 0.9495798349380493, 0.9547325372695923, 0.9666666388511658, 0.9428571462631226, 0.9430894255638123, 0.9549180269241333, 0.9662446975708008, 0.9583333134651184, 0.9533898234367371], 'acc': [0.9539999961853027, 0.9380000233650208, 0.9520000219345093, 0.9580000042915344, 0.9440000057220459, 0.9459999799728394, 0.9539999961853027, 0.9520000219345093, 0.949999988079071, 0.9380000233650208], 'rec': [0.9387755393981934, 0.922448992729187, 0.9469387531280518, 0.9469387531280518, 0.9428571462631226, 0.9469387531280518, 0.9510204195976257, 0.9346938729286194, 0.9387755393981934, 0.918367326259613], 'f1pos': [0.9523809523809523, 0.9358178053830227, 0.9508196721311476, 0.9567010309278351, 0.9428571428571428, 0.945010183299389, 0.9529652351738241, 0.9502074688796679, 0.9484536082474228, 0.9355509355509356], 'f1neg': [0.9555125725338491, 0.9400386847195358, 0.953125, 0.9592233009708738, 0.9450980392156862, 0.9469548133595286, 0.9549902152641879, 0.9536679536679538, 0.9514563106796117, 0.9402697495183044]}, 'test': {'loss': [0.3129979372024536, 0.3289146423339844, 0.3063347339630127, 0.31043338775634766, 0.30388087034225464, 0.37295767664909363, 0.3603467047214508, 0.2958783805370331, 0.289208322763443, 0.32644015550613403], 'auc': [0.9537814855575562, 0.9517806768417358, 0.9643857479095459, 0.9477791786193848, 0.9579831957817078, 0.9289716482162476, 0.94197678565979, 0.9785914421081543, 0.9687875509262085, 0.9513805508613586], 'prec': [0.918367326259613, 0.9333333373069763, 0.9200000166893005, 0.918367326259613, 0.9200000166893005, 0.8392857313156128, 0.8518518805503845, 0.97826087474823, 0.8867924809455872, 0.8846153616905212], 'acc': [0.8999999761581421, 0.8799999952316284, 0.9100000262260437, 0.8999999761581421, 0.9100000262260437, 0.8700000047683716, 0.8700000047683716, 0.9300000071525574, 0.8999999761581421, 0.8899999856948853], 'rec': [0.8823529481887817, 0.8235294222831726, 0.9019607901573181, 0.8823529481887817, 0.9019607901573181, 0.9215686321258545, 0.9019607901573181, 0.8823529481887817, 0.9215686321258545, 0.9019607901573181], 'f1pos': [0.9, 0.8749999999999999, 0.9108910891089109, 0.9, 0.9108910891089109, 0.8785046728971961, 0.8761904761904761, 0.9278350515463919, 0.9038461538461539, 0.8932038834951457], 'f1neg': [0.9, 0.8846153846153846, 0.9090909090909091, 0.9, 0.9090909090909091, 0.8602150537634408, 0.8631578947368421, 0.9320388349514563, 0.8958333333333333, 0.8865979381443299]}}\n"
     ]
    }
   ],
   "source": [
    "hyper_list = [{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555} ,\n",
    "{'n_hidden': 1, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0005834382440750414, 'dropout': 0.48787589688314237, 'threshold': 0.7622802703643394} ,\n",
    "{'n_hidden': 10, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 5.56666629147225e-05, 'dropout': 0.6376447344086704, 'threshold': 0.2292333004984978} ,\n",
    "{'n_hidden': 1, 'n_units': 37, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0006521686023472134, 'dropout': 0.2862399358316623, 'threshold': 0.8171312422835556} ,\n",
    "{'n_hidden': 7, 'n_units': 6, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0013414232848841775, 'dropout': 0.6061345542965496, 'threshold': 0.3544065415833555} ,\n",
    "{'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 0.00032005478454991034, 'learning_rate': 8.446138571374139e-05, 'dropout': 0.8630165121896143, 'threshold': 0.52263525360727} ,\n",
    "{'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.3395203701483684, 'threshold': 0.3544065415833555} ,\n",
    "{'n_hidden': 9, 'n_units': 54, 'kernel_regularizer': 2.0678394943630145e-05, 'learning_rate': 6.444708842918975e-05, 'dropout': 0.48726918436401034, 'threshold': 0.7581930049210841} ,\n",
    "{'n_hidden': 3, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0005834382440750414, 'dropout': 0.8662788901341136, 'threshold': 0.3056244326515286} ,\n",
    "{'n_hidden': 1, 'n_units': 19, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 0.12198972576711176, 'dropout': 0.6376447344086704, 'threshold': 0.3318972015983154} ,\n",
    "{'n_hidden': 1, 'n_units': 33, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.1697866286480649, 'threshold': 0.48870052508813955} ,\n",
    "{'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 0.019189432566192953, 'learning_rate': 0.0007970799285309584, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114} ,\n",
    "{'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 0.00023198013313174801, 'learning_rate': 7.015653993460166e-05, 'dropout': 0.11174948366441749, 'threshold': 0.31665717730358856} ,\n",
    "{'n_hidden': 1, 'n_units': 37, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0006521686023472134, 'dropout': 0.048525896101412724, 'threshold': 0.27265874826006264} ,\n",
    "{'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 1.214740272740579e-05, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.3923850775943305} ,\n",
    "{'n_hidden': 2, 'n_units': 11, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 0.00039626071240343367, 'dropout': 0.8568014240464407, 'threshold': 0.10873515186137227} ,\n",
    "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.3544065415833555} ,\n",
    "{'n_hidden': 1, 'n_units': 6, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.6061345542965496, 'threshold': 0.3318972015983154} ,\n",
    "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.48870052508813955} ,\n",
    "{'n_hidden': 1, 'n_units': 34, 'kernel_regularizer': 0.0005069808946385528, 'learning_rate': 0.021296428950781383, 'dropout': 0.8662788901341136, 'threshold': 0.3544065415833555} ,\n",
    "{'n_hidden': 9, 'n_units': 8, 'kernel_regularizer': 0.0004416315616331735, 'learning_rate': 0.00020655775452336643, 'dropout': 0.5039281376641517, 'threshold': 0.4195383350653711} ,\n",
    "{'n_hidden': 3, 'n_units': 18, 'kernel_regularizer': 6.428253014937363e-05, 'learning_rate': 7.015653993460166e-05, 'dropout': 0.7860688156703391, 'threshold': 0.1325102969894017} ,\n",
    "{'n_hidden': 9, 'n_units': 44, 'kernel_regularizer': 1.8148712421091602e-05, 'learning_rate': 0.002389600411589469, 'dropout': 0.48787589688314237, 'threshold': 0.3814482156016251} ,\n",
    "{'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 9.988480575396382e-05, 'dropout': 0.5499841382958144, 'threshold': 0.27265874826006264} ,\n",
    "{'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 0.019189432566192953, 'learning_rate': 0.0007970799285309584, 'dropout': 0.29992301899310914, 'threshold': 0.25478675673102114} ,\n",
    "{'n_hidden': 1, 'n_units': 64, 'kernel_regularizer': 0.0007793010643204281, 'learning_rate': 0.0003795051194324455, 'dropout': 0.7010918993652716, 'threshold': 0.3056244326515286} ,\n",
    "{'n_hidden': 1, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.1325102969894017} ,\n",
    "{'n_hidden': 7, 'n_units': 25, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 9.988480575396382e-05, 'dropout': 0.5499841382958144, 'threshold': 0.27265874826006264} ,\n",
    "{'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 0.0007582681194411686, 'dropout': 0.37889332143468546, 'threshold': 0.3923850775943305} ,\n",
    "{'n_hidden': 3, 'n_units': 44, 'kernel_regularizer': 0.0005489530832949244, 'learning_rate': 5.3577811386515144e-05, 'dropout': 0.6571075663804247, 'threshold': 0.3056244326515286} ,\n",
    "{'n_hidden': 7, 'n_units': 14, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0006955292277375732, 'dropout': 0.8959560597731027, 'threshold': 0.3544065415833555} ,\n",
    "{'n_hidden': 3, 'n_units': 56, 'kernel_regularizer': 2.1928352979948724e-05, 'learning_rate': 0.0012602416730026099, 'dropout': 0.0686413637905968, 'threshold': 0.4195383350653711} ,\n",
    "{'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 0.0017899630867169573, 'learning_rate': 0.0005834382440750414, 'dropout': 0.7860688156703391, 'threshold': 0.7347123009931876} ,\n",
    "{'n_hidden': 1, 'n_units': 16, 'kernel_regularizer': 5.525232660110897e-05, 'learning_rate': 0.6699399471845631, 'dropout': 0.6061345542965496, 'threshold': 0.1325102969894017} ,\n",
    "{'n_hidden': 5, 'n_units': 33, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.001598284971321218, 'dropout': 0.20902702870023096, 'threshold': 0.3544065415833555} ,\n",
    "{'n_hidden': 10, 'n_units': 44, 'kernel_regularizer': 0.00017468471331461992, 'learning_rate': 0.00012248524389842045, 'dropout': 0.3395203701483684, 'threshold': 0.8929033947032199} ,\n",
    "{'n_hidden': 2, 'n_units': 42, 'kernel_regularizer': 0.007872536731619325, 'learning_rate': 4.9567167614517136e-05, 'dropout': 0.08891052102456992, 'threshold': 0.2292333004984978} ,\n",
    "{'n_hidden': 2, 'n_units': 44, 'kernel_regularizer': 1.1874693702272719e-05, 'learning_rate': 0.0007970799285309584, 'dropout': 0.20896209008607705, 'threshold': 0.8929033947032199} ,\n",
    "{'n_hidden': 1, 'n_units': 25, 'kernel_regularizer': 0.12254567223517063, 'learning_rate': 0.003763855754353628, 'dropout': 0.8662788901341136, 'threshold': 0.3544065415833555} ,\n",
    "{'n_hidden': 1, 'n_units': 58, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.0003795051194324455, 'dropout': 0.20896209008607705, 'threshold': 0.383494481615293} ,\n",
    "{'n_hidden': 7, 'n_units': 14, 'kernel_regularizer': 4.8737081393919183e-05, 'learning_rate': 4.9567167614517136e-05, 'dropout': 0.545499225702559, 'threshold': 0.2292333004984978}]\n",
    "\n",
    "adjusting_best = [{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58},\n",
    "    {'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-06, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58} ,\n",
    "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-07, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58} ,\n",
    "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-08, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58} ,\n",
    "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-09, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58},\n",
    "{'n_hidden': 3, 'n_units': 34, 'kernel_regularizer': 1.7151027836617935e-10, 'learning_rate': 7.785232427941312e-05, 'dropout': 0.20896209008607705, 'threshold': 0.58}\n",
    "]\n",
    "\n",
    "hyper_dict2 = {}\n",
    "\n",
    "\n",
    "for index, h in enumerate(adjusting_best):\n",
    "    h['activation'] = 'relu'\n",
    "    print('current_para_set', h)\n",
    "    hyper_dict2['set_' + str(index)] = test_models(n_hidden = h['n_hidden'],\n",
    "                                                   n_units = h['n_units'],\n",
    "                                                   activation = h['activation'],\n",
    "                                                   kernel_regularizer = h['kernel_regularizer'],\n",
    "                                                   learning_rate=h['learning_rate'],\n",
    "                                                   dropout=h['dropout'],\n",
    "                                                   threshold=h['threshold'],\n",
    "                                                   )\n",
    "\n",
    "\n",
    "    print(hyper_dict2['set_' + str(index)])\n",
    "\n",
    "# selected\n",
    "# {'n_hidden': 5, 'n_units': 33, 'kernel_regularizer': 1.7151027836617935e-05, 'learning_rate': 0.001598284971321218, 'dropout': 0.20902702870023096, 'threshold': 0.3544065415833555} ,\n",
    "\n",
    "# set_18 mbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_0\n",
      "train\n",
      "loss 0.18028023838996887 +- 0.004744269054919387\n",
      "auc 0.9876558721065521 +- 0.0010186824484891702\n",
      "prec 0.9602644979953766 +- 0.0031894709254382606\n",
      "acc 0.9541999995708466 +- 0.0030177264587789247\n",
      "rec 0.9457142889499665 +- 0.004347356610199673\n",
      "f1pos 0.9528881426194976 +- 0.003131360908950238\n",
      "f1neg 0.9554368479658146 +- 0.0029151112008980852\n",
      "test\n",
      "loss 0.3377810657024384 +- 0.012021977738088352\n",
      "auc 0.9414365589618683 +- 0.005732373852178217\n",
      "prec 0.895765197277069 +- 0.011098424390781791\n",
      "acc 0.8769999980926514 +- 0.006506408018812931\n",
      "rec 0.8607843220233917 +- 0.00944890943340301\n",
      "f1pos 0.8771939436774773 +- 0.006136925300779733\n",
      "f1neg 0.8765700939458714 +- 0.007203247441562328\n",
      "set_1\n",
      "train\n",
      "loss 0.17324524223804474 +- 0.008209747993733361\n",
      "auc 0.9894917666912079 +- 0.0012522933638226788\n",
      "prec 0.960617071390152 +- 0.002616009408631397\n",
      "acc 0.9535999953746795 +- 0.001995553322881881\n",
      "rec 0.9440816342830658 +- 0.0033904553966774406\n",
      "f1pos 0.9522315757830476 +- 0.0020802958520988015\n",
      "f1neg 0.9548884892503381 +- 0.0019208008991281232\n",
      "test\n",
      "loss 0.31845695078372954 +- 0.008809416469056081\n",
      "auc 0.9503601491451263 +- 0.002880878451832451\n",
      "prec 0.8977753937244415 +- 0.01195743956836198\n",
      "acc 0.8849999904632568 +- 0.0068718438658641684\n",
      "rec 0.8764705955982208 +- 0.01095624419286555\n",
      "f1pos 0.8860751617815348 +- 0.006447822684214115\n",
      "f1neg 0.883645065202504 +- 0.007582661498678847\n",
      "set_2\n",
      "train\n",
      "loss 0.17474488765001298 +- 0.0034105713782262644\n",
      "auc 0.9899431765079498 +- 0.0007243437899055923\n",
      "prec 0.9568293273448945 +- 0.003895364152884176\n",
      "acc 0.9527999937534333 +- 0.002831571009215234\n",
      "rec 0.9465306043624878 +- 0.004489794764859585\n",
      "f1pos 0.9515670992543013 +- 0.0029393963931905096\n",
      "f1neg 0.9539643449157212 +- 0.00273684976457285\n",
      "test\n",
      "loss 0.3348468512296677 +- 0.011116044056056078\n",
      "auc 0.945838350057602 +- 0.005691095326380395\n",
      "prec 0.8872715532779694 +- 0.006265364873540501\n",
      "acc 0.8870000004768371 +- 0.005174727130767938\n",
      "rec 0.8921568691730499 +- 0.007307411255620106\n",
      "f1pos 0.8895059663126116 +- 0.005147908355800227\n",
      "f1neg 0.8843191478271704 +- 0.0052757481030300795\n",
      "set_3\n",
      "train\n",
      "loss 0.17220135331153869 +- 0.0031631523555983115\n",
      "auc 0.9900800287723541 +- 0.0006928045055131293\n",
      "prec 0.9567612946033478 +- 0.004317804712574334\n",
      "acc 0.9550000011920929 +- 0.0017701224392479892\n",
      "rec 0.9514285683631897 +- 0.003357542355692495\n",
      "f1pos 0.9539755595925896 +- 0.0017641829167019238\n",
      "f1neg 0.955970074747621 +- 0.0017877133032913235\n",
      "test\n",
      "loss 0.3304836541414261 +- 0.01151544952725663\n",
      "auc 0.9462985396385193 +- 0.004994990674659327\n",
      "prec 0.8908655643463135 +- 0.01099501269749241\n",
      "acc 0.8859999895095825 +- 0.00871779716994447\n",
      "rec 0.886274516582489 +- 0.013330769175601406\n",
      "f1pos 0.887862817537804 +- 0.00883598972765906\n",
      "f1neg 0.8838882499387374 +- 0.008698288447590092\n",
      "set_4\n",
      "train\n",
      "loss 0.18058513402938842 +- 0.004303492019622193\n",
      "auc 0.9886802911758423 +- 0.0008668891988011077\n",
      "prec 0.9540546178817749 +- 0.002978585634626208\n",
      "acc 0.9521999955177307 +- 0.0023371416267171778\n",
      "rec 0.9481632709503174 +- 0.002655677095518324\n",
      "f1pos 0.9510785166239977 +- 0.00238420745006901\n",
      "f1neg 0.9532694038395431 +- 0.002293613776060967\n",
      "test\n",
      "loss 0.31606643199920653 +- 0.010209684921549462\n",
      "auc 0.9570828378200531 +- 0.0033014558238674383\n",
      "prec 0.9040704667568207 +- 0.007640721460488777\n",
      "acc 0.8950000047683716 +- 0.005821417673030553\n",
      "rec 0.8901960849761963 +- 0.017093720328353136\n",
      "f1pos 0.8957407822214233 +- 0.006887630774187575\n",
      "f1neg 0.8939100912365486 +- 0.004945920633384661\n",
      "set_5\n",
      "train\n",
      "loss 0.18283483982086182 +- 0.005546344913331219\n",
      "auc 0.9890332221984863 +- 0.0008196259922986731\n",
      "prec 0.9556198000907898 +- 0.0028300699019487285\n",
      "acc 0.9486000061035156 +- 0.0021715312673010506\n",
      "rec 0.938775509595871 +- 0.0034419343264786148\n",
      "f1pos 0.947076403483134 +- 0.0022677110905173522\n",
      "f1neg 0.9500336639929531 +- 0.0020893831097676153\n",
      "test\n",
      "loss 0.32073928117752076 +- 0.008605520117810751\n",
      "auc 0.954541826248169 +- 0.004429338916263821\n",
      "prec 0.9050874352455139 +- 0.01285274034139764\n",
      "acc 0.8959999978542328 +- 0.006000001121453358\n",
      "rec 0.8921568691730499 +- 0.008889849475833088\n",
      "f1pos 0.8976362416193184 +- 0.005447803647111187\n",
      "f1neg 0.8940640257726606 +- 0.0068420787385957815\n"
     ]
    }
   ],
   "source": [
    "for key, value in hyper_dict2.items():\n",
    "    print(key)\n",
    "    for tt, value2 in value.items():\n",
    "        print(tt)\n",
    "        for metric, value3 in value2.items():\n",
    "            print(metric, np.array(value3).mean(), '+-',sem(np.array(value3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tryings_stuff2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
